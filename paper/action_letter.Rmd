---
title       : "Selecting relevant moderators with Bayesian regularized meta-regression"
authors     : "masked"
journal     : "Research Synthesis Methods"
editor      : "Pigott"
manuscript  : "RSM-04-2022-0056"
class       : "draft"
bibliography      : ["meta-analysis.bib"]
output      : papaja::revision_letter_pdf
---

```{r, message=FALSE, warning=FALSE}
library(revise)
library(tidySEM)
out <- readRDS("output.RData")
read_manuscript("manuscript.Rmd")

```

Dear Dr. `r rmarkdown::metadata$editor`,

Thank you for considering our manuscript for publication in _`r rmarkdown::metadata$journal`_.
We appreciate the thorough and critical comments you solicited,
which helped strengthen the argumentation, clarify the structure, and add nuance.
We have attempted to address all comments as thoroughly as possible.
All changes are explained below.
We hope that you will find the quality of the work to be sufficiently improved.

Yours sincerely,

the authors

# Associate Editor {-}

\RC{The authors introduce their implementation of a regularisation approach for estimation and variable selection in meta-regression, which seems a timely contribution given the danger of overfitting. Some more detail or elaboration would be helpful in some places.}

We have rewritten all sections of the paper and restructured some of them. Furthermore, in addressing all reviewer comments (see below), we have added detail and elaboration in many places. We hope that this addresses this comment.

\RC{please consider whether the title is appropriate, or whether (something like) "Selecting..." or "Selection of..." might be better.}

We appreciate the suggestion and have changed the title to:

*Selecting relevant moderators with Bayesian regularized meta-regression*

\RC{in abstract and Introduction, please consider being more concrete here: it might be good to mention here already that this is about penalisation/LASSO/horseshoe priors in order to give the reader a better idea.}

To address this comment and others,
we have restructured the Introduction to be more to the point.
We still provide the same background information in a more abbreviated form at a later point.
With regard to the specific request to mention penalisation/LASSO/horseshoe priors in the Abstract and Introduction, we have made the following changes. The Abstract now reads:

`r get_revision("lassoabstract")`

The Introduction now states:

`r get_revision("lassointroduction")`

\RC{on p.16 (Section "design factors"):
what were the total numbers of variables (or "noise moderators")?}

We see now that we did not report clearly enough how many design factors there were and what they were.
To address this comment and avoid confusion, we have rewritten this paragraph as follows:

`r get_revision("designfactors")`

\RC{regarding the 3rd comment by Reviewer 1: in the context of variable selection, the so-called "penalized complexity priors" (here: exponential priors for the heterogeneity standard deviation) might play a role.}

To address the 3d comment by Reviewer 2, we have included the following paragraph in the revision:

`r get_revision("tauprior")`

# Reviewer 1

\RC{The manuscript provides an interesting approach based on regularization to selection of covariates in meta-regression, when the number of covariates is relevant. The Authors focus on a Bayesian strategy.
Although I found the topic interesting, the study needs some additional investigations and the text needs to be substantially modified in order to provide a clear and exhaustive picture of the proposal.
My comments and questions are listed below.}

We thank the Reviewer for their constructive comments, which substantially helped improve the quality of the manuscript.
We have tried to address them all in turn, as detailed below.

\RC{The introduction is too long in terms of explanation of the meta-regression and selection of covariates problems, with many repetitions. For example, end of page 4 and beginning of page 5 include many repetitions about between-study heterogeneity. Other parts, instead, require a substantially deeper explanation.}

To address this comment and others,
we have restructured the Introduction to be more to the point.
We still provide the same background information in a more abbreviated form at a later point.
We have elaborated on all the points that were raised by the Reviewers (see our responses to the other comments).

\RC{Which theories useful for variable selection do you refer to in page 5 ? The description is extremely vague.}

We have attempted to clarify this section by invoking an additional reference and rewriting it as follows:

`r get_revision("theory")`

\RC{The topic of the study is variable selection from a Bayesian point of view: but the description is so short. Just a few lines about the novelty of the paper in page 6, with no clear description of the proposal and with the reference to pema package without even the explanation of the name of the package.}

To address this comment, we have rewritten the opening paragraph to clearly state the problem addressed in this paper, and the proposed solution:

`r get_revision("openingparagraph")`

\RC{Statistical underpinnings: Again, I found a lot of repetitions in the text.}

We have merged several sections to avoid repetition in the text.

\RC{Probably, in (1) you mean $\theta$ and not $\Theta$? As you have $\theta$ int the text. Why do you use theta and not beta, as you do for the meta-regression model in (7)? The notation would be simplified and would be more consistent.}

We have made the suggested change and now use the intercept $\beta_0$ in place of $\Theta$. We have also simplified the notation of the estimated population effect to $\hat{\beta_0}$.

\RC{The sentence in lines 132-133 is not clear.}

We have attempted to clarify this sentence as follows:

`r get_revision("informationdistribution")`

\RC{line 147: probably the explanation of the error term should be anticipated where the error term appears for the first time.}

To address this comment, we have moved the explanation of the "error term" to the paragraph where it is first discussed.

\RC{line 152: “to estimate this model “ has a preferable statistical sound than “to solve this model”}

We have made the suggested change.

\RC{lines 163- 166 describe some features of regularization….but regularization is introduced later, from line 167.}

To address this comment, we have moved this paragraph about regularization into the section on regularized regression.

\RC{Regularized regression + Bayesian estimation
I found both the sections very short and lacking of many details.}

Our aim is to provide a high-level conceptual introduction of the method's underpinnings,
not to replicate the many excellent technical publications that already exist on these methods. 
To address this comment, we have edited all technical sections for greater clarity and detail. Additionally, we explicitly direct the reader to appropriate introductory texts that go more in depth:

`r get_revision("reftibshirani")`

And:

`r get_revision("refmcelreath")`

\RC{what about the existing literature using lasso for meta-regression?}

We were not aware of any existing literature at the time of writing;
most likely because several such papers were only published after we wrote our draft - but we thank the Reviewer for encouraging us to search the literature again. Based on our findings, we now include the following paragraph:

`r get_revision("earlierpapers")`

\RC{In addition, I expected some references about the theory of LASSO given the relevance of the instrument in the literature.}

To address this comment, we now cite the following theoretical paper:

`r get_revision("reftibshirani")`

\RC{line 183-184: “other penalties exist”: ok, so discuss about, indicate advantages and disadvantages of the other solutions; why do you not focus on them in the study? If you do not, just explain why.}

We respectfully feel that indicating advantages and disadvantages of other penalties is out of scope for the present paper.
The only reason we explain the LASSO penalty is to explain how regularization works.
Since we ultimately do not use a frequentist solution with shrinkage penalties, there is little point to explaining other penalties in detail.

To address this comment, we now clarify why we explain the LASSO penalty:

`r get_revision("whylasso")`

\RC{Bayesian estimation: I think that some additional methodological details and additional explanations about priors and their comparison to the classical LASSO would help the reader.}

As requested, we have added this information. These sections now read:

`r get_revision("morepriors")`

<!-- Second, the frequentist LASSO shrinks some coefficients to be exactly zero, thereby performing automatic variable selection.  -->
<!-- This is not the case for its Bayesian counterpart, which always provides a continuous posterior probability distribution. -->
<!-- Variable selection is then performed by determining which parameters have a high probability density assigned to near-zero values. -->
<!-- Third, classical penalized regression approaches rely on optimization while the Bayesian framework generally relies on Markov Chain Monte Carlo (MCMC) sampling to obtain the posterior distribution. As a result, Bayesian estimation tends to be slower compared to classical estimation. -->

\RC{Why details relevant for the Bayesian estimation are included in the “Implementation” section? I mean lasso prior and horseshoe prior, that is, details useful to understand the methodology.}

We agree with the Reviewer that this information would be better positioned in another section.
To address this comment, we have restructured the paper; the information about priors is now incorporated into the *Bayesian estimation* section, and the *Implementation* section now exclusively relates to the `pema` R-package.

\RC{Can you comment about the choice of the numerical values in the priors at the end of page 11?}

We now explain this as follows:

`r get_revision("defaultvalues")`

Note that the results of our simulation study again validate the fact that these are sensible defaults.

\RC{line 221: “this extension”: which one?}

We recognize that this choice of words was confusing.
The regularized horseshoe prior is an extension of the horseshoe prior.
To address this comment, we now explain this in the text:

`r get_revision("horseshoe")`

\RC{line 225: “This is accomplished by..” Why?}

We have rephrased this so that it is clearer that the formula explains how this is achieved. The section now reads:

`r get_revision("expectedmods")`

\RC{line 240: “values are reasonable in most applications” Why? Can you prove this?}

We thank the Reviewer for pointing out that this statement raises more questions than it answers. To address this Reviewer comment, we have moved this statement to the Discussion and slightly rephrased it to clarify that it is supported by the results of the present study. It now reads:

`r get_revision("sensitivityanalysis")`

\RC{lines 244-251 seems to be pertinent to the methodology of the proposal not to implementation.}

We agree with the Reviewer that the writing in this section was too methodological. To address this comment, we have made two changes: This paragraph has been moved to the section on Bayesian estimation, and we have rewritten it as follows:

`r get_revision("intervals")`

\RC{More in general, I do not like to see a mixture of methodology and R code. I think I would be more useful for  the reader to have sections describing the proposed approach and the underlying methodology (something that is not present at the emano) and then having an appendix about the code.}

We see that a stricter separation of methodology and code could aid readability.
Therefore, we have addressed this comment by relegating all code to the Implementation section.

We do believe that there are advantages to having code in the paper and not in an appendix,
as the intended readership for this paper consists of those who wish to conduct a Bayesian regularized meta-analysis. For this purpose, it is important that readers understand how to translate the methodological principles to a model specified in the `pema` package.
We hope that the current revision strikes a reasonable compromise.

\RC{line 278: “x==0” could be substituted by “x is equal to 0” as this is a text, not a source code.}

We have made the suggested change.

\RC{Intercept.
I don’t think it is so relevant to deserve a section. Why not inserting a line in the section devoted to the code?}

This section is important because omitting the intercept from the model has very different implications for regularized meta-regression than for non-regularized models.

To clarify this, we have rewritten this section to focus on the implications of including or excluding the intercept.
The line explaining *how* to include or exclude the intercept has been moved to the Implementation paragraph, consistent with the Reviewer's request to relegate code to a separate section:

`r get_revision("intercept")`

\RC{Performance indicators, Design factors, Predictive performance
* You split the data in training set and test set, for each iteration. So, how do you account for the variability associated to the choice of the test set (as done for example in cross-validation)?}

We respectfully believe that this comment is based on a misunderstanding of the procedure.
The Reviewer's comment seems to suggest that we split a finite sample into a training and testing set. This is not the case. 

Instead, we simulate both data sets from the same known data generating model.
Each sample from this model is thus independent and identically distributed (IID).
The distinction between training and testing is arbitrary.
This is different from real data analyses, where the available dataset is finite, and training and testing data are drawn from that finite dataset - thus violating the assumption of IID.

In a real dataset, if one case has very unusual values (i.e., an outlier),
the model could be biased if this case ends up in the training data, and the performance metrics could be biased if this case ends up in the testing data.
In our simulation study, outliers have exactly equal probabilities of appearing in the training or testing data - and as we are not sampling from a finite dataset, the probability of having an outlier in the training data is independent from having one in the testing data.

Their effect on the estimated parameters is known as "Monte Carlo error" and averages out across replications of the simulation.

\RC{I’m surprised by the large number fo linea devoted to the explanation of such a basic indicator like R2. In addition, it is well known to be optimistic, and thus the adjusted version I usually preferable. Why don’t you evaluate the method using other indicators, such as the adjusted r2, AIC, or BIC?}

We respectfully feel that it is important to define how we operationalized each outcome metric, and why we use it.
In fact, it seems that our explanation was not sufficiently clear,
because the outcome metric is not the basic indicator R2,
as the Reviewer has apparently understood,
but rather, a different type of R2 that is suitable for assessing overfitting.

To address this comment, we have tried to explain the outcome metric, $R^2_{test}$ more clearly:

`r get_revision("rsquared")`

With regard to the Reviewer's suggestion to use AIC or BIC:
the reason we do not use these is because they are comparative fit indices, only suitable for comparing a set of models; we have only a single model and AIC or BIC do not speak to the absolute fit of a single model.

The reason we do not use adjusted $R^2$ is because it is an approximation of $R^2_{test}$ that can be calculated on the training data.
Since we can simulate testing data from a known model,
we do not need to *estimate* $R^2_{test}$ - we can *calculate* it directly.

\RC{line 344: I suppose you mean the estimate of sensitivity and specificity.}

We have rephrased this as suggested.

\RC{line 368: 100 datasets only? Probably, there is a typo, as 100 is the number of datasets included in the test set, if I’m not wrong. In any case, 100 is a very low number of replicates for a simulation study, I suggest to increase at least to the common value equal to 1,000.}

As reported, we used 100 replications per simulation condition because these simulations are very computationally demanding,
and increasing the number of replications adds only marginal accuracy gains.
In total, we thus simulated 194.400 cases.
Many of the analyses marginalize over some design factors,
thus we always have a very large sample size.

We have calculated the runtime required to conduct 1,000 replications per condition,
and this amounts to 8160 computer-hours, or 340 days on a single core.
This is exceptionally expensive and environmentally unfriendly,
and the marginal gains are minimal.
We thus strongly prefer to keep the number of replications at 100,
but we defer to the Editor in this matter.

\RC{line 385: do you mean “Table” 1?}

Thank you for pointing out this omission; we have corrected it.

\RC{line 405: “the” most negative?}

We corrected this, and have rephrased the sentence for clarity.

\RC{lines 449-452 are not necessary as bias and variance are known concepts.}

We respectfully feel that it is prudent to define how all outcome metrics are operationalized.
Even though the general concepts of bias and variance may be known to most readers,
we cannot assume that they will know exactly how these are calculated in the context of this simulation study - particularly for the broad readership of a journal like Research Synthesis Methods.
See also our response to the comment about defining $R^2_{test}$

\RC{However, in Tables I can see only bias, and not information about variance.}

This is correct; we had not reported these statistics for two reasons:

1. Unlike bias, they must be computed on aggregate across replications of each simulation condition. This results in a strange situation where each condition has only one outcome value. But it is probably fine to interpret these effect sizes as descriptive statistics.
2. The differences between algorithms in variance of $\tau^2$ were negligible. We did not want to devote two tables to, essentially, a matrix of zeroes.

To address this comment, we now describe the results in the text, and provide the requested tables as online Supplements:

`r get_revision("variancebeta")`

And:

`r get_revision("variancetau")`

\RC{lines 469-470: there is a white space}

We thank the Reviewer for pointing this out;
a fragment of a sentence was cut here.
We restored the original text.

\RC{Could you please avoid writing pema::bonapersona and use instead dataset titled bonapersona in the R pema package? This is not a list of lines code, but a text.}

We have done as requested.

\RC{you have 440 experiments in the dataset: do you mean 440 study included in the meta-regression? If so, the number is much. Larger than the scenarios evaluated in the simulations.}

This observation is correct, but this is not a problem. 
The method is validated for up to 100 studies,
so it should also be valid for >100 studies,
as a larger sample size gives more power.

The reason we used these data is because we were able to secure permission from the original author to include them in the R-package.
This means we can make the examples reproducible, requiring only the `pema` package.
We feel that it adds a lot to the paper to present a reproducible real-data example based on a novel data set.
To address this comment,
we now acknowledge in the text that this sample is relatively large compared to the sample sizes of the simulation study:

`r get_revision("sample400")`

\RC{Finally, There are many typos and inconsistencies throughout the text (e.g., data set and dataset, random effect and randoms-effects, …).}

We have proofread and spell checked the manuscript, and corrected all mistakes found.

# Reviewer 2

\RC{Comments to the Author
The authors investigated selection of moderators for the meta-regression, which is especially useful to explain the sources of heterogeneity between trials. They proposed the use of regularizing priors for meta-regression within a Bayesian framework. They shared publicly available R package "pema" which relies on rstan, which helps practitioners to use the proposed methods. I think the contribution of the paper and R package is important. However, I have some comments.}

We thank the Reviewer for their encouraging words and thought-provoking comments, which we feel have helped improve the quality of the manuscript.

## Major points

\RC{1) I think it is important to note some limitations of meta-regression. For example, the use of moderators might "break" the randomization, when the studies analyzed are randomized controlled trials. This is because it is not possible to randomize patients to one moderator, see Thompson and Higgins (2002) section 3 for further discussion of limitations of meta-regression of clinical trials.}

We agree with the Reviewer that there are important limitations to meta-regression, and Thompson & Higgins provide an excellent analysis thereof.
<!-- Limitations -->
<!-- i) multicolinearity -->
<!-- ii) ecological fallacy -->
<!-- iii) restriction of range -->
<!-- iv) multicolinearity -->
<!-- v) something specific to clinical trials?? -->
<!-- vi) data availability -->
<!-- vii) small sample size, limited generalizability -->
<!-- PITFALLS -->
<!-- i) curse of dimensionality: small sample size, multicolinearity -->
<!-- ii) overfitting -->
<!-- iii) variable selection -->
<!-- iv) variable selection -->
Most of the limitations and pitfalls they discuss come down to two fundamental points which are already addressed throughout our paper: 1) the curse of dimensionality and its implications for multicolinearity (limitations i, iv, vii and pitfalls i, ii, iii, and iv), and 2) the ecological fallacy (limitation ii and iv).
They further address three singular points which cannot be attributed to the two fundamental points:
Limitation iii relates to the problem of restriction of range in moderators (which we do not address),
Limitation vi relates to limited data availability (which we address in our discussion of missing data),
and Limitation v makes a point about regression to the mean in RCTs that we fail to grasp.

Since our method is a statistical solution to point 1),
we devote most attention to it.
We also acknowledge most of the other points.
However, we respectfully feel that it is out of scope for the present paper to devote further attention to limitations of meta-regression that our method does *not* address;
especially since Thompson & Higgins already did so.

To address this comment, we have referenced the Thompson & Higgins paper in several places, and we have added the following sentences to our Recommendations for Applied Researchers:

`r get_revision("metareglimits")`

\RC{2) Important references are missing, in which the use of regularization or penalization approaches for the meta-analysis were investigated, although not the same purpose as in the current paper. For example, in Chung et al (2013) regularizing prior was used for the heterogeneity parameter to avoid boundary estimates, and in Günhan et al (2020) regularizing priors were used for the heterogeneity parameter and treatment effect parameter to deal with data sparsity. Finally and most importantly, Röver et al (2021) reviewed different use of regularizing priors and provided some guidance. Note that in the mentioned references, the term of weakly informative priors are used instead of regularizing priors (also see an earlier reference by Gelman (2006) in a hierarchical model context). Mention of these publications and relationship to the present paper can help the reader.}

First of all, we thank the Reviewer for suggesting these references, which had eluded our literature search.
We now cite them in relevant places in the manuscript.

Second of all, we agree that the use of weakly informative priors for heterogeneity parameters was insufficiently discussed in the previous version of our manuscript, and have now added a paragraph on that topic (see our response to the next comment).

Third of all, we feel that it is important to stress that the use of weakly informative priors (WIP) for heterogeneity parameters serves a categorically different purpose than the use of regularizing priors for regression coefficients - even if the term "regularization" has been used for both in prior literature.
The purpose of WIP for heterogeneity parameters is to aid model convergence and avoid boundary estimates.
The purpose of regularizing priors for regression coefficients is to perform variable selection.
We believe that the newly added paragraph sufficiently clarifies these distinct uses of "regularization" (see response to next comment).

\RC{3) An important part of Bayesian random-effects meta-analysis is the specification of the prior for the heterogeneity parameter tau. Can you include more specifications on the prior choice for tau and influence of the choice to the results (if there is any).}

We agree with the Reviewer that the prior specification for the heterogeneity parameter $tau$ is important and should be addressed in the text.
To address this comment, we have included the following paragraph in the revision:

`r get_revision("tauprior")`

\RC{4) In the abstract and in the main text, it states "We present a simulation study to validate the performance of BRMA relative to state-of-the-art meta-regression (RMA)". What does RMA refer to here, is it "Random effect Meta-Analysis using REML"? If yes, I think the term random-effect meta-analysis (or meta-regression) using RMLE is more clear than state-of-the-art meta-analysis (or meta-regression).}

We have made the suggested correction. Both in the Abstract and in the text, the first time we introduce RMA, we introduce it as "random effects meta-analysis using restricted maximum likelihood".

## Minor points

\RC{Line 385: "see 1": I think Table is missing}

Thank you; we have corrected this.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
