[{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"tutorial-requirements","dir":"Articles","previous_headings":"","what":"Tutorial Requirements","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Follow instructions : https://posit.co/download/rstudio-desktop/ Open RStudio. Now, run setup script workshop. window labeled “Console”, paste code press [Enter]: Load data R usual, check can use workshop exercises running: check_data(your_data_object)","code":"source(\"https://raw.githubusercontent.com/cjvanlissa/meta_workshop/refs/heads/master/check_function.R\")"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"pool","dir":"Articles","previous_headings":"","what":"Meta-Analysis in R","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"essence Meta-Analysis pooling effect sizes get overall effect size estimate studies. pooling effect sizes Meta-Analysis, two basic approaches: Fixed-Effect-Model, Random-Effects-Model (Borenstein et al. 2009). fixed effect model assumes one true effect size exists population; random effects model assumes true effect varies (normally distributed). models require effect size, dispersion (variance) estimate study. meta-analyses, ’ll use metafor package (Viechtbauer 2010). However, notice load different package, metaforest. reason contains example data tutorial, loads metafor package turn.","code":"library(metaforest)"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"using-your-own-data","dir":"Articles","previous_headings":"Meta-Analysis in R","what":"Using Your Own Data","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"want, can conduct examples data. Note interactive questions won’t work case (correct answers based curry data). want use data, can use function check_data() help make sure data suitable tutorial examples. Install function running: , apply function data object running check_data(your_data_object). give helpful suggestions make easier follow tutorial data: Note message suggests rename effect size column, called d curry dataset, yi. Let’s :","code":"source(\"https://raw.githubusercontent.com/cjvanlissa/meta_workshop/refs/heads/master/check_function.R\") #> ✔ Looks like you're all set to do the workshop! df <- curry check_data(df) #> There is no column named 'yi' in your data. If you have an effect size column, it will be easier to do the tutorial if you rename it, using syntax like this: #>    #>   names(df)[which(names(df) == 'youreffectsize')] <- 'yi' #>    #>   If you do not yet have an effect size column, you may need to compute it first. Run ?metafor::escalc to see the help for this function. names(df)[which(names(df) == 'd')] <- 'yi'"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"using-different-demo-data","dir":"Articles","previous_headings":"Meta-Analysis in R","what":"Using Different Demo Data","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"data challenge, can use different demo dataset complete tutorial. Two suggestions :","code":"metaforest::fukkink_lont pema::bonapersona"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"fixedef","dir":"Articles","previous_headings":"","what":"Fixed Effect Model","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"idea behind fixed-effects-model fixed-effects-model assumes observed effect sizes stem single true population effect (Borenstein et al. 2009). calculate overall effect, therefore average effect sizes, give studies greater precision higher weight. Precision relates fact studies smaller Standard Error provide accurate estimates true population effect. weighing, use inverse variance 1/σ̂k21/\\hat\\sigma^2_k study kk. calculate weighted average studies, fixed effect size estimator θ̂F\\hat\\theta_F: θ̂F=∑k=1Kθ̂k/σ̂k2∑k=1K1/σ̂k2 \\begin{equation} \\hat\\theta_F = \\frac{\\sum\\limits_{k=1}^K \\hat\\theta_k/ \\hat\\sigma^2_k}{\\sum\\limits_{k=1}^K 1/\\hat\\sigma^2_k} \\end{equation} examples assume already dataset calucated effects SE study. curry data set, inside metaforest package, . dataset already contains effect sizes variances, can directly use rma function.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"fixed-effects-model-with-rma","dir":"Articles","previous_headings":"Fixed Effect Model","what":"Fixed-effects Model with rma","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"rma() function many arguments, can accessed typing ?rma console metafor package loaded, selecting function pressing F1. table important arguments code: Let’s conduct first fixed-effects-model Meta-Analysis. give results analysis simple name m. now see summary results Meta-Analysis, including total number included studies (k) overall effect, confidence interval p-value Q-test heterogeneity Formative Assessment  big overall effect? True false: significant proportion variability effect sizes attributable heterogeneity, rather sampling error. TRUEFALSE","code":"m <- rma(yi = df$yi,     # The yi-column of the df, which contains Cohen's d          vi = df$vi,    # The vi-column of the df, which contains the variances          method = \"FE\") # Run a fixed-effect model m #> Fixed-Effects Model (k = 56) #>  #> I^2 (total heterogeneity / total variability):   64.95% #> H^2 (total variability / sampling variability):  2.85 #>  #> Test for Heterogeneity: #> Q(df = 55) = 156.9109, p-val < .0001 #>  #> Model Results: #>  #> estimate      se    zval    pval   ci.lb   ci.ub       #>   0.2059  0.0219  9.4135  <.0001  0.1630  0.2487  ***  #>  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"random","dir":"Articles","previous_headings":"","what":"Random-Effects-Model","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"can use fixed-effect-model can assume included studies tap one true effect size. practice hardly ever case: interventions may vary certain characteristics, sample used study might slightly different, methods. can assume many small, random, uncorrelated variations true effect sizes, appropriate assumption cases might true effect size follows normal distribution. Idea behind Random-Effects-Model Random-Effects-Model, want account assumption population effect size normally distributed (Schwarzer, Carpenter, Rücker 2015). fixed-effect-model assumes observed effect size θ̂k\\hat\\theta_k individual study kk deviates true effect size θF\\theta_F, reason estimate burdened (sampling) error ϵk\\epsilon_k. θ̂k=θF+ϵk\\hat\\theta_k = \\theta_F + \\epsilon_k random-effects-model assumes , addition, second source error ζk\\zeta_k.second source error introduced fact even true effect size θk\\theta_k study kk also part -arching distribution true effect sizes mean μ\\mu(Borenstein et al. 2009). illustration parameters random-effects-model formula random-effects-model therefore looks like : θ̂k=μ+ϵk+ζk\\hat\\theta_k = \\mu + \\epsilon_k + \\zeta_k calculating random-effects-model meta-analysis, therefore also take error ζk\\zeta_k account. , estimate variance distribution true effect sizes, denoted τ2\\tau^{2}, tau2. several estimators τ2\\tau^{2}, implemented metafor.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"random-effects-model-with-rma","dir":"Articles","previous_headings":"","what":"Random-Effects Model with rma","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"can re-use code fixed-effects-model simply remove method = \"FE\" argument conduct random-effects analysis default REML estimator τ2\\tau^2: Formative Assessment  big overall effect? estimated heterogeneity?  caused fact random-effects models assign equal weight studies, including small ones, tend biased.","code":"m_re <- rma(yi = df$yi,     # The yi-column of the df, which contains Cohen's d             vi = df$vi)    # The vi-column of the df, which contains the variances m_re #>  #> Random-Effects Model (k = 56; tau^2 estimator: REML) #>  #> tau^2 (estimated amount of total heterogeneity): 0.0570 (SE = 0.0176) #> tau (square root of estimated tau^2 value):      0.2388 #> I^2 (total heterogeneity / total variability):   67.77% #> H^2 (total variability / sampling variability):  3.10 #>  #> Test for Heterogeneity: #> Q(df = 55) = 156.9109, p-val < .0001 #>  #> Model Results: #>  #> estimate      se    zval    pval   ci.lb   ci.ub       #>   0.2393  0.0414  5.7805  <.0001  0.1581  0.3204  ***  #>  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"metareg","dir":"Articles","previous_headings":"","what":"Meta-Regression","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"idea behind meta-regression conventional regression, specify model predicting dependent variable yy, across i_i participants, based values pp predictor variables, xi1…xipx_{i1} \\dots x_{ip}. residual error referred ϵi\\epsilon_i. standard regression equation therefore looks like : yi=β0+β1x1i+...+βpxpi+ϵiy_i=\\beta_0 + \\beta_1x_{1i} + ...+\\beta_px_{pi} + \\epsilon_i meta-regression, want estimate effect size θ\\theta several studies k_k, function -studies moderators. two sources heterogeneity: sampling error, ϵk\\epsilon_k, -studies heterogeneity, ζk\\zeta_k regression looks like : θk=β0+β1x1k+...+βpxpk+ϵk+ζk\\theta_k = \\beta_0 + \\beta_1x_{1k} + ... + \\beta_px_{pk} + \\epsilon_k + \\zeta_k might seen estimating effect size θk\\theta_k study kk regression model, two extra terms equation, ϵk\\epsilon_k ζk\\zeta_k. terms can also found equation random-effects model. two terms signify two types independent errors cause regression prediction imperfect. first one, ϵk\\epsilon_k, sampling error effect size study deviates “true” effect. second one, ζk\\zeta_k, denotes even true effect size study sampled overarching distribution effect sizes. equation includes fixed effects (β\\beta coefficients) well random effects (ζk\\zeta_k), model used meta-regression often called mixed-effects-model.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"testing-moderators-significance","dir":"Articles","previous_headings":"Meta-Regression","what":"Testing Moderators’ Significance","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"evaluate statistical significance predictor, conduct t-test (Z-test) β\\beta-weight. t=βSEβ t=\\frac{\\beta}{SE_{\\beta}} provides pp-value telling us variable significantly predicts effect size differences regression model.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"assessing-meta-regression-model-fit","dir":"Articles","previous_headings":"Meta-Regression","what":"Assessing Meta-Regression Model Fit","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"conventional regression, R2R^2 commonly used quantify percentage variance data explained model, percentage total variance (0-100%). measure commonly used, many researchers know interpret , can also calculate R2R^2 analog meta-regression using formula: R2=τ̂REM2−τ̂MEM2τ̂REM2R_2=\\frac{\\hat\\tau^2_{REM}-\\hat\\tau^2_{MEM}}{\\hat\\tau^2_{REM}} τ̂REM2\\hat\\tau^2_{REM} estimated total heterogenetiy based random-effects-model τ̂REM2\\hat\\tau^2_{REM} total heterogeneity mixed-effects regression model. NOTE however, R2R^2 refers variance explained observed data. linear regression, predictors add, better model explain observed data. can decrease generalizability model, process called overfitting: ’re capturing noise dataset, true effects exist real world. meta-regression, estimate heterogeneity may differ, may actually see R2R^2 increasing predictors - samples often small, mindful overfitting nonetheless.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"meta-regression-in-r","dir":"Articles","previous_headings":"Meta-Regression","what":"Meta-regression in R","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Meta-regressions can conducted R using rma function metafor. First, let’s conduct regression single categorical predictor, donorcode, two levels: Anxious Typical: default, metafor::rma() recode factor variables make one group reference category create dummies groups. syntax created binary indicator variable (dummy variable) people fall Typical category. estimated model intercept - population effect size Anxious category, serves reference category , effect donorcodeTypical dummy, tells us difference two groups’ population effects. Formative Assessment  expected population effect size Typical group? expected population effect size Anxious group?","code":"m_cat <- rma(yi ~ donorcode, vi = vi, data = df) m_cat #>  #> Mixed-Effects Model (k = 56; tau^2 estimator: REML) #>  #> tau^2 (estimated amount of residual heterogeneity):     0.0567 (SE = 0.0177) #> tau (square root of estimated tau^2 value):             0.2382 #> I^2 (residual heterogeneity / unaccounted variability): 67.81% #> H^2 (unaccounted variability / sampling variability):   3.11 #> R^2 (amount of heterogeneity accounted for):            0.49% #>  #> Test for Residual Heterogeneity: #> QE(df = 54) = 155.0054, p-val < .0001 #>  #> Test of Moderators (coefficient 2): #> QM(df = 1) = 1.5257, p-val = 0.2168 #>  #> Model Results: #>  #>                   estimate      se    zval    pval    ci.lb   ci.ub     #> intrcpt             0.0829  0.1332  0.6222  0.5338  -0.1781  0.3439     #> donorcodeTypical    0.1730  0.1401  1.2352  0.2168  -0.1015  0.4476     #>  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"continuous-predictors","dir":"Articles","previous_headings":"Meta-Regression > Meta-regression in R","what":"Continuous Predictors","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Imagine want check proportion male participants associated effect size. variable sex contains information. can use predictor meta-regression: Formative Assessment  True false: significant effect sex. TRUEFALSE","code":"m_reg <- rma(yi ~sex,              vi = vi,              data = df) m_reg #>  #> Mixed-Effects Model (k = 56; tau^2 estimator: REML) #>  #> tau^2 (estimated amount of residual heterogeneity):     0.0544 (SE = 0.0173) #> tau (square root of estimated tau^2 value):             0.2333 #> I^2 (residual heterogeneity / unaccounted variability): 66.50% #> H^2 (unaccounted variability / sampling variability):   2.98 #> R^2 (amount of heterogeneity accounted for):            4.53% #>  #> Test for Residual Heterogeneity: #> QE(df = 54) = 149.5878, p-val < .0001 #>  #> Test of Moderators (coefficient 2): #> QM(df = 1) = 2.1607, p-val = 0.1416 #>  #> Model Results: #>  #>          estimate      se    zval    pval    ci.lb   ci.ub     #> intrcpt    0.0648  0.1253  0.5168  0.6053  -0.1808  0.3104     #> sex        0.0050  0.0034  1.4699  0.1416  -0.0017  0.0116     #>  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"multiple-meta-regression","dir":"Articles","previous_headings":"Meta-Regression > Meta-regression in R","what":"Multiple Meta-Regression","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Previously, considered scenario use one predictor β1x1\\beta_1x_1 meta-regression. add one predictor, ’re using multiple meta-regression. multiple meta-regression use several moderators (variables) predict outcome (effect sizes). look back general meta-regression formula defined , actually see formula already provides us feature βnxpk\\beta_nx_{pk} part. , parameter pp denotes can include pp predictors/variables meta-regression, making multiple meta-regression. include predictors previous exercises mutliple meta-regression follows: Now, let’s create model includes potential moderators effect size: Formative Assessment  number studies included second multiple meta-regression (predictors)? number parameters second multiple meta-regression (predictors)?","code":"m_multi <- rma(yi ~ sex + donorcode,                vi = vi,                data = df) m_multi #>  #> Mixed-Effects Model (k = 56; tau^2 estimator: REML) #>  #> tau^2 (estimated amount of residual heterogeneity):     0.0551 (SE = 0.0175) #> tau (square root of estimated tau^2 value):             0.2347 #> I^2 (residual heterogeneity / unaccounted variability): 66.89% #> H^2 (unaccounted variability / sampling variability):   3.02 #> R^2 (amount of heterogeneity accounted for):            3.42% #>  #> Test for Residual Heterogeneity: #> QE(df = 53) = 148.6166, p-val < .0001 #>  #> Test of Moderators (coefficients 2:3): #> QM(df = 2) = 3.0602, p-val = 0.2165 #>  #> Model Results: #>  #>                   estimate      se     zval    pval    ci.lb   ci.ub     #> intrcpt            -0.0337  0.1626  -0.2074  0.8357  -0.3523  0.2849     #> sex                 0.0043  0.0035   1.2310  0.2183  -0.0025  0.0111     #> donorcodeTypical    0.1361  0.1421   0.9579  0.3381  -0.1424  0.4146     #>  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 mods <- c(\"sex\", \"age\", \"location\", \"donorcode\", \"interventioncode\", \"controlcode\", \"recipients\", \"outcomecode\")  res_multi2 <- rma(as.formula(paste(\"yi ~\", paste(mods, collapse = \"+\"))), vi = vi, data = df)  res_multi2 #>  #> Mixed-Effects Model (k = 56; tau^2 estimator: REML) #>  #> tau^2 (estimated amount of residual heterogeneity):     0.0038 (SE = 0.0076) #> tau (square root of estimated tau^2 value):             0.0618 #> I^2 (residual heterogeneity / unaccounted variability): 12.55% #> H^2 (unaccounted variability / sampling variability):   1.14 #> R^2 (amount of heterogeneity accounted for):            93.30% #>  #> Test for Residual Heterogeneity: #> QE(df = 26) = 34.1905, p-val = 0.1303 #>  #> Test of Moderators (coefficients 2:30): #> QM(df = 29) = 105.3506, p-val < .0001 #>  #> Model Results: #>  #>  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"optional-pitfalls-of-meta-regression","dir":"Articles","previous_headings":"Meta-Regression","what":"Optional: Pitfalls of Meta-Regression","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Skip section pressed time.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"multicollinearity","dir":"Articles","previous_headings":"Meta-Regression > Optional: Pitfalls of Meta-Regression","what":"Multicollinearity","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"performing analysis , saw warning message: warning tells us moderators conveyed exactly information, therefore, ’s possible include model. example, look following subset data. Imagine entire dataset: Note variable age unique values study, first two second two studies completely identical variables location, interventioncode, controlcode. dataset, thus possible determine unique effect three variables (overlap completely). example (multi)colinearity. Even worrying: column outcomecode almost value study. , column redundant model’s intercept! longer variable, constant. stands, one study distinct outcomecode. add dummy variable - dummy uniquely identifies one study (means model perfectly reproduce value)!","code":"#> Warning: Redundant predictors dropped from the model. df[c(5, 24, 28, 46), c(\"age\", \"location\", \"interventioncode\", \"controlcode\", \"outcomecode\")] #>      age location   interventioncode      controlcode outcomecode #> 5  21.00   Canada Prosocial Spending        Self Help       Other #> 24 20.00   Canada Prosocial Spending        Self Help   Happiness #> 28 18.55      USA   Acts of Kindness Neutral Activity       Other #> 46 29.95      USA   Acts of Kindness Neutral Activity       Other"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"overfitting","dir":"Articles","previous_headings":"Meta-Regression > Optional: Pitfalls of Meta-Regression","what":"Overfitting","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"better understand risks (multiple) meta-regression models, understand concept overfitting. Overfitting occurs build statistical model fits data closely. essence, means build statistical model can predict data hand well, performs bad predicting future data never seen . happens model assumes variation data stems true “signal” data, fact model random noise (Hastie, Tibshirani, Friedman 2009). result, statistical model produces false positive results: sees relationships none. Illustration overfitted model vs. model good fit Regression methods, usually utilize minimization maximization procedures Ordinary Least Squares Maximum Likelihood estimation, can prone overfitting (Hastie, Tibshirani, Friedman 2009). Unfortunately, risk building non-robust model, produces false-positive results, even higher go conventional regression meta-regression (J. P. T. Higgins Thompson 2004). several reasons : Meta-Regression, sample often small, can use synthesized data analyzed studies kk. meta-analysis aims comprehensive overview evidence, additional data can “test” well regression model can predict high low effect sizes. meta-regressions, deal potential presence effect size heterogeneity. Imagine case two studies different effect sizes non-overlapping confidence intervals. Every variable different values different studies might potential explanation effect size difference find, seems straightforward explanations spurious (J. P. T. Higgins Thompson 2004). Meta-regression , multiple meta-regression particular, make easy “play around” predictors. can test numerous meta-regression models, include many predictors remove attempt explain heterogeneity data. approach course tempting, often found practice, , meta-analysts, want find significant model explains effect sizes differ (J. Higgins et al. 2002). However, behavior massively increases risk spurious findings (J. P. T. Higgins Thompson 2004), can change parts model indefinitely find significant model, likely overfitted (.e., mostly models statistical noise). guidelines proposed avoid excessive false positive rate building meta-regression models: Minimize number investigated predictors priori. Predictor selection based predefined scientific theoretical questions want answer meta-regression. evaluating fit meta-regression model, prefer models achieve good fit less predictors. Use fit indices balance fit parsimony, Akaike Bayesian information criterion, determine model retain compare several models. number studies low (likely case), want compute significance predictor, can use Knapp-Hartung adjustment obtain reliable estimates (J. Higgins et al. 2002), specifying test = \"knha calling rma().","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"penalized-meta-regression","dir":"Articles","previous_headings":"","what":"Penalized Meta-Regression","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Meta-regression can used account potentially relevant -studies differences. However, previously saw number candidate moderators often high relative number studies. introduces risks overfitting, spurious results, model non-convergence. Bayesian Regularized Meta-Analysis (BRMA) overcomes challenges, selecting relevant moderators shrinking small regression coefficients towards zero regularizing (LASSO horseshoe) priors (Van Lissa, van Erp, Clapper 2023). method suitable many potential moderators, known beforehand relevant. Let’s use Bayesian regularized meta-regression (BRMA) select relevant moderators! First, load necessary packages. running model locally multi-core machine, can set options(mc.cores = 4). runs 4 MCMC chains parallel, giving multiple independent sampling chains keeping estimation time relatively short.","code":"library(pema) library(tidySEM) library(ggplot2) options(mc.cores = 4)"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"two-level-model","dir":"Articles","previous_headings":"Penalized Meta-Regression","what":"Two-level model","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"First, simplicity, run two-level BRMA model, ignoring fact certain effect sizes come study.","code":"df_brma <- df[, c(\"yi\", \"vi\", mods)]"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"two-level-model-with-the-lasso-prior","dir":"Articles","previous_headings":"Penalized Meta-Regression > Two-level model","what":"Two-level model with the lasso prior","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"start running penalized meta-analysis using lasso prior. Compared horseshoe prior, lasso easier use two hyperparameters set. However, lighter tails lasso can result large coefficients shrunken much towards zero thereby leading potentially bias compared regularized horseshoe prior. lasso prior, need specify degrees freedom df scale. default 1. degrees freedom determines chi-square prior specified inverse-tuning parameter. Increasing degrees freedom allow larger values inverse-tuning parameter, leading less shrinkage. Increasing scale parameter also result less shrinkage. influence hyperparameters can visualized implemented shiny app, can called via shiny_prior(), calling plot(sample_prior()).","code":"set.seed(1) fit_lasso <- brma(yi ~ .,                   data = df_brma,                   vi = \"vi\",                   method = \"lasso\",                   prior = c(df = 1, scale = 1),                   mute_stan = FALSE)"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"assessing-convergence-and-interpreting-the-results","dir":"Articles","previous_headings":"Penalized Meta-Regression > Two-level model > Two-level model with the lasso prior","what":"Assessing convergence and interpreting the results","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"can request results using summary function. interpret results, need ensure MCMC chains converged posterior distribution. Two helpful diagnostics provided summary number effective posterior samples n_eff potential scale reduction factor Rhat. n_eff estimate number independent samples posterior. Ideally, ratio n_eff total samples close 1 possible. Rhat compares - within-chain estimates ideally close 1 (indicating chains mixed well). values n_eff Rhat far ideal values, can try increasing number iterations iter argument. default, brma function runs four MCMC chains 2000 iterations , half discarded burn-. result, total 4000 iterations available posterior summaries based. help, non-convergence might indicate problem model specification. satisfied convergence, can continue looking posterior summary statistics. summary function provides posterior mean estimate effect moderator. Since Bayesian penalization automatically shrink estimates exactly zero, additional criterion needed determine moderators selected model. Currently, done using 95% credible intervals, moderator selected zero excluded interval. summary denoted asterisk moderator. Also note summary statistics tau2, (unexplained) residual -studies heterogeneity. Formative Assessment  many significant moderator effects ? TRUEFALSE True false: significant residual heterogeneity. TRUEFALSE","code":"sum <- summary(fit_lasso) sum$coefficients[, c(\"mean\", \"sd\", \"2.5%\", \"97.5%\", \"n_eff\", \"Rhat\")]"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"optional-three-level-model-with-horseshoe-prior","dir":"Articles","previous_headings":"Penalized Meta-Regression","what":"Optional: Three-level model with Horseshoe Prior","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"default prior brma() horseshoe prior, performed best simulation studies. Moreover, can take account fact effect sizes might come study fitting three-level model follows:","code":"df_brma_threelevel <- df[, c(\"yi\", \"vi\", \"study_id\", mods)] set.seed(7) fit_hs <- brma(yi ~ .,                data = df_brma_threelevel,                vi = \"vi\",                study = \"study_id\")"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"more-information","dir":"Articles","previous_headings":"Penalized Meta-Regression > Optional: Three-level model with Horseshoe Prior","what":"More Information","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Reference: Van Lissa, van Erp, Clapper (2023) elaborate tutorial, see vignette.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"random-forest-meta-regression","dir":"Articles","previous_headings":"","what":"Random Forest Meta-Regression","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"BRMA comparable multiple meta-regression, main difference shrinks coefficients towards zero eliminate irrelevant moderators. alternative machine learning-informed approach moderator selection use tree-based methods. section, use MetaForest - random forest-based approach - analyze data select relevant moderators (Van Lissa 2020).","code":"# Load the metaforest package library(metaforest) #> Loading required package: metafor #> Loading required package: Matrix #> Loading required package: metadat #> Loading required package: numDeriv #>  #> Loading the 'metafor' package (version 4.8-0). For an #> introduction to the package please type: help(metafor) #> Loading required package: ranger #> Loading required package: data.table"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"check-convergence","dir":"Articles","previous_headings":"Random Forest Meta-Regression","what":"Check Convergence","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"random forest model, important determine many trees needed get stable results. Convergence indicated stabilization cumulative mean squared --bag prediction error (MSEoob), function number trees model. run analysis high number trees, pick smaller number trees, model also seen converged, speed computationally heavy steps, replication model tuning. re-examine convergence final model. Note can account multiple effect sizes per study MetaForest well:  model converges around 2500 trees.","code":"# Run model with many trees to check convergence set.seed(74) check_conv <- MetaForest(yi~., data = df_brma_threelevel, study = \"study_id\", whichweights = \"random\", num.trees = 10000) # Plot convergence trajectory plot(check_conv)"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"model-tuning","dir":"Articles","previous_headings":"Random Forest Meta-Regression","what":"Model Tuning","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Machine learning models often hyperparameters must tuned get good performance specific learning task. can tune model using R package caret. tuning parameters, consider three types weights (uniform, fixed-, random-effects), number candidate variables split 2–6, minimum node size 2–6. select model smallest root mean squared prediction error (RMSE) final model, based 5-fold clustered cross-validation. Clustered cross-validation means effect sizes study always included fold, account dependency data. Note number folds exceed number clusters data. Moreover, number clusters small, one might resort specifying number folds clusters. Model tuning computationally intensive might take long time. Based root mean squared error, best combination tuning parameters fixed-effect weights, three candidate variables per split, minimum three cases per terminal node.","code":"# Load caret library(caret) # Set up 5-fold clustered CV grouped_cv <- trainControl(method = \"cv\", index = groupKFold(df_brma_threelevel$study_id, k = 5))  # Set up a tuning grid tuning_grid <- expand.grid(whichweights = c(\"random\", \"fixed\", \"unif\"), mtry = 2:6, min.node.size = 2:6) # X should contain only retained moderators, clustering variable, and vi X <- df_brma_threelevel[, c(\"study_id\", \"vi\", mods)] # Train the model set.seed(3) mf_cv <- train(y = df_brma_threelevel$yi, x = X, study = \"study_id\", # Name of the clustering variable method = ModelInfo_mf(), trControl = grouped_cv, tuneGrid = tuning_grid, num.trees = 2500) # Best model tuning parameters mf_cv$results[which.min(mf_cv$results$RMSE), ] #>    whichweights mtry min.node.size      RMSE  Rsquared       MAE     RMSESD #> 32        fixed    3             3 0.2760904 0.2936028 0.2240218 0.08613236 #>    RsquaredSD      MAESD #> 32  0.2951682 0.06719638"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"interpreting-the-results","dir":"Articles","previous_headings":"Random Forest Meta-Regression","what":"Interpreting the Results","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"object returned train already contains final model, estimated best combination tuning parameters. model positive estimate explained variance new data, Roob2=.19R^2_{oob} = .19. Now, proceed interpreting moderator effects, examining variable importance partial dependence plots. Variable importance determined randomly permuting variable turn (loses meaningful association outcome), assessing much model’s predictive performance diminishes random permutation. important variable, model performance drops permutation. Partial dependence plots show model’s predicted values different levels one variable, averaging values variables. Thus, can think bivariate marginal association. Formative Assessment  negative permutation importance donorcode indicate? Prosocial SpendingOtherActs Kindness intervention lowest associated effect size? Randomly permuting variable improves model fitThis variable positive effect outcome (effect size)Randomly permuting variable diminishes model fitThis variable negative effect outcome (effect size)","code":"# Extract final model final <- mf_cv$finalModel # Extract R^2_oob from the final model r2_oob <- final$forest$r.squared # Plot variable importance VarImpPlot(final)  # Sort the variable names by importance ordered_vars <- names(final$forest$variable.importance)[ order(final$forest$variable.importance, decreasing = TRUE)]  # Plot partial dependence PartialDependence(final, vars = ordered_vars, rawdata = TRUE, pi = .95)"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"more-information-1","dir":"Articles","previous_headings":"Random Forest Meta-Regression","what":"More Information","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Reference: Van Lissa (2020) elaborate tutorial, see open access book chapter MetaForest.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"bayesian-evidence-synthesis","dir":"Articles","previous_headings":"","what":"Bayesian Evidence Synthesis","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Heterogeneity presents fundamental challenge research synthesis methods (J. P. T. Higgins, Thompson, Spiegelhalter 2009). studies assess informative hypothesis, differ fundamental ways preclude aggregation conventional meta-analysis methods, Bayesian Evidence Synthesis (BES) can used. Consider, example, situation arises number moderators large relative number studies, moderators redundant (see Meta-Regression chapter ). PBF (Van Lissa, Kuiper, Clapper 2023) can used instead random-effects meta-analysis assumptions likely violated. Tutorial illustrates use PBF cases. start day, conducted random- fixed effects meta-analyses using function rma() metafor package. required yi vi columns. Now, perform PBF analysis using pbf() function bain package, accepts arguments. First, load required packages.","code":"library(bain)"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"repeating-the-rma","dir":"Articles","previous_headings":"Bayesian Evidence Synthesis","what":"Repeating the RMA","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"Let’s repeat previous random effects meta-analysis, examine funnel plot distribution effect sizes:  effect size 0 means helping others increased happiness, even little bit. Bayes Factor, test informative hypothesis (just directional null hypothesis) helping increases happiness one particular study, example first row dataset: results indicate first study provides 1508 times evidence favor informative hypothesis (helping increases happiness) , BF=1508.434BF = 1508.434.","code":"res_rma <- rma(yi = yi, vi = vi, data = df) funnel(res_rma) set.seed(1) bain(x = c(\"yi\" = df$yi[1]), # Select the first effect size      Sigma = matrix(df$vi[1], 1, 1), # Make covariance matrix      hypothesis = \"yi > 0\", # Define informative hypothesis      n = df$n1i[1]+df$n2c[1]) # Calculate total sample size #> Bayesian informative hypothesis testing for an object of class numeric: #>  #>    Fit   Com   BF.u  BF.c     PMPa  PMPb  PMPc  #> H1 0.999 0.500 1.999 1508.434 1.000 0.667 0.999 #> Hu                                  0.333       #> Hc 0.001 0.500 0.001                      0.001 #>  #> Hypotheses: #>   H1: yi>0 #>  #> Note: BF.u denotes the Bayes factor of the hypothesis at hand versus the unconstrained hypothesis Hu. BF.c denotes the Bayes factor of the hypothesis at hand versus its complement. PMPa contains the posterior model probabilities of the hypotheses specified. PMPb adds Hu, the unconstrained hypothesis. PMPc adds Hc, the complement of the union of the hypotheses specified."},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"product-bayes-factor","dir":"Articles","previous_headings":"Bayesian Evidence Synthesis","what":"Product Bayes Factor","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"now perform PBF analysis, using pbf() method numeric input (see ?pbf). interface similar rma(), specifically designed applications PBF applied meta-analytic datasets. yi vi arguments rma(). Additional argument ni used construct prior approximate Bayes factors (Hoijtink et al. 2019). Importantly, hypothesis argument determines informative hypotheses tested. resulting output shows product Bayes factors informative hypothesis (PBF column), well study-specific Bayes factors (remaining columns, trimmed keep document tidy). PBF favor hypothesis high (7.45 69 zeroes), body evidence provides consistent support notion helping increases happiness. Formative Assessment  negative Bayes factor indicate, Bayes factor comparing informative hypothesis Hi complement Hc? Negative Bayes factors don’t existThe evidence Hc outweighs evidence HiThe evidence Hi outweighs evidence HcThe evidence Hi outweighs evidence Hc, effect negative True false: Bayes factors must exceed one (BF > 1) obtain PBF also exceeds one (PBF > 1) TRUEFALSE  last question relates generalizability. perform normal fixed- random-effects meta-analysis, rely sampling theory assume effect size represents random sample population possible effect sizes. assumption justified, can claim pooled effect size generalized population effect sizes randomly sampled . Thus, pooled effect size tells us something future studies. course, assumption questionable, due publication bias, fact researchers build upon previous work! practice, meta-analysis might generalizable ’d like. PBF rely sampling theory make assumptions random sampling population. result, can interpreted quantitative summary evidence within present sample studies. incorrect interpret PBF saying anything population; interpret summary statistic studies hand. course, said conventional meta-analysis, assumption random sampling violated.","code":"set.seed(1) res_pbf <- pbf(yi = df$yi,                vi = df$vi,                ni = sum(df$n1i+df$n2c),                hypothesis = \"y > 0\")"},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"more-information-2","dir":"Articles","previous_headings":"Bayesian Evidence Synthesis","what":"More Information","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"References: Van Lissa, Kuiper, Clapper (2023), Van Lissa et al. (2020) elaborate tutorial Product Bayes Factor, see paper, press Research Synthesis Methods.","code":""},{"path":"https://cjvanlissa.github.io/pema/articles/meta-analysis_tutorial.html","id":"credit","dir":"Articles","previous_headings":"Bayesian Evidence Synthesis","what":"Credit","title":"Tutorial: Machine Learning-Informed Meta-Analysis","text":"tutorial partly based book Meta-Analysis R: Hands-Guide, also recommended reading: Harrer, M., Cuijpers, P., Furukawa, T.., & Ebert, D.D. (2021). Meta-Analysis R: Hands-Guide. Boca Raton, FL London: Chapmann & Hall/CRC Press. ISBN 978-0-367-61007-4.","code":""},{"path":[]},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"packages","dir":"Articles","previous_headings":"","what":"Packages","title":"Conducting a Bayesian Regularized Meta-analysis","text":"First, load necessary packages. addition, running model locally multi-core machine, can set options(mc.cores = 4). ensures different MCMC chains run parallel, making estimation faster.","code":"library(pema) library(tidySEM) library(ggplot2) options(mc.cores = 4)"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Conducting a Bayesian Regularized Meta-analysis","text":"application, work bonapersona data (Bonapersona et al., 2019). data codebook can found . First, read data make sure relevant variables correct type.","code":"descriptives(bonapersona)[, c(\"name\", \"type\", \"n\", \"unique\", \"mean\", \"sd\", \"v\")] #>                    name      type   n unique     mean     sd     v #> 1                   exp   integer 734    411  222.644 111.88    NA #> 2                    id   integer 734    212   99.402  60.03    NA #> 3                author character 734    209       NA     NA 0.990 #> 4                  year   integer 734     21 2011.014   4.45    NA #> 5               journal character 734     69       NA     NA 0.945 #> 6               species    factor 734      3       NA     NA 0.433 #> 7         strainGrouped    factor 734     15       NA     NA 0.762 #> 8                origin    factor 734      4       NA     NA 0.617 #> 9                   sex    factor 734      3       NA     NA 0.373 #> 10              ageWeek   numeric 721     25   13.333   5.75    NA #> 11                model    factor 734      6       NA     NA 0.667 #> 12           mTimeStart   integer 734     12    2.357   2.51    NA #> 13             mTimeEnd   integer 734     18   12.802   4.39    NA #> 14            mHoursAve   numeric 734     33   54.076  47.73    NA #> 15         mCageGrouped    factor 734      4       NA     NA 0.485 #> 16           mLDGrouped    factor 734      5       NA     NA 0.469 #> 17          mRepetition    factor 734      7       NA     NA 0.446 #> 18      mControlGrouped    factor 734      4       NA     NA 0.069 #> 19          hit2Grouped    factor 734      3       NA     NA 0.493 #> 20    testAuthorGrouped    factor 734     27       NA     NA 0.894 #> 21        testLDGrouped    factor 734      4       NA     NA 0.295 #> 22     varAuthorGrouped    factor 734     47       NA     NA 0.914 #> 23               waterT    factor 734      9       NA     NA 0.125 #> 24           waterTcate    factor 734      5       NA     NA 0.123 #> 25         freezingType    factor 734     10       NA     NA 0.221 #> 26     retentionGrouped    factor 734      5       NA     NA 0.520 #> 27     directionGrouped    factor 734      5       NA     NA 0.648 #> 28 effectSizeCorrection   integer   5      2    1.000   0.00    NA #> 29               cut_nC   integer  37      2    1.000   0.00    NA #> 30         n_notRetriev   integer   5      2    1.000   0.00    NA #> 31                   nC   numeric 734     25   10.719   4.07    NA #> 32                meanC   numeric 734    412  167.110 112.13    NA #> 33                  sdC   numeric 734    499   73.919  62.43    NA #> 34                  seC   numeric 734    209   23.314  19.61    NA #> 35                   nE   numeric 734     25   11.200   4.15    NA #> 36                meanE   numeric 734    430  163.209 114.28    NA #> 37                  sdE   numeric 734    500   76.684  62.99    NA #> 38                  seE   numeric 734    206   23.554  19.54    NA #> 39             dataFrom    factor 734      4       NA     NA 0.288 #> 40        seqGeneration    factor 734      3       NA     NA 0.487 #> 41             baseline    factor 734      4       NA     NA 0.412 #> 42           allocation    factor 734      3       NA     NA 0.170 #> 43              housing    factor 734      2       NA     NA 0.000 #> 44             blindExp    factor 734      4       NA     NA 0.429 #> 45              control    factor 734      4       NA     NA 0.043 #> 46               outAss    factor 734      4       NA     NA 0.456 #> 47             outBlind    factor 734      5       NA     NA 0.654 #> 48              incData    factor 734      3       NA     NA 0.219 #> 49            waterTNum   numeric  48      8   22.292   1.37    NA #> 50                 each   integer 734    734  367.500 212.03    NA #> 51          mTimeLength   integer 734     19   10.446   5.61    NA #> 52 speciesStrainGrouped    factor 734     16       NA     NA 0.762 #> 53            blindRand    factor 734      3       NA     NA 0.317 #> 54                 bias   numeric 734      9    2.884   0.75    NA #> 55                   tV    factor 734     60       NA     NA 0.929 #> 56              anxiety   integer 734      2    0.486   0.50    NA #> 57            sLearning   numeric 734      2    0.277   0.45    NA #> 58           nsLearning   numeric 734      2    0.143   0.35    NA #> 59               social   integer 734      2    0.063   0.24    NA #> 60             multiply   integer 734      2   -0.272   0.96    NA #> 61               noMeta   numeric 734      2    0.031   0.17    NA #> 62               domain    factor 734      6       NA     NA 0.662 #> 63        directionQual   numeric 617      4    0.183   0.58    NA #> 64                   yi   numeric 734    726    0.243   1.34    NA #> 65                   vi   numeric 734    728    0.263   0.36    NA"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"impute-missings","dir":"Articles","previous_headings":"","what":"Impute missings","title":"Conducting a Bayesian Regularized Meta-analysis","text":"","code":"bonapersona$ageWeek[is.na(bonapersona$ageWeek)] <- median(bonapersona$ageWeek, na.rm = TRUE)"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"moderators","dir":"Articles","previous_headings":"","what":"Moderators","title":"Conducting a Bayesian Regularized Meta-analysis","text":"application, use smaller selection moderators Bonapersona et al. (2019).","code":"datsel <- bonapersona[ , c(\"yi\", \"vi\", \"author\", \"mTimeLength\", \"year\", \"model\", \"ageWeek\", \"strainGrouped\", \"bias\", \"species\", \"domain\", \"sex\")]"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"two-level-model","dir":"Articles","previous_headings":"","what":"Two-level model","title":"Conducting a Bayesian Regularized Meta-analysis","text":"First, simplicity, run two-level model ignoring fact certain effect sizes come study.","code":"dat2l <- datsel dat2l[[\"author\"]] <- NULL"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"two-level-model-with-the-lasso-prior","dir":"Articles","previous_headings":"Two-level model","what":"Two-level model with the lasso prior","title":"Conducting a Bayesian Regularized Meta-analysis","text":"start running penalized meta-analysis using lasso prior. Compared horseshoe prior, lasso easier use two hyperparameters set. However, lighter tails lasso can result large coefficients shrunken much towards zero thereby leading potentially bias compared regularized horseshoe prior. lasso prior, need specify degrees freedom df scale. default 1. degrees freedom determines chi-square prior specified inverse-tuning parameter. Increasing degrees freedom allow larger values inverse-tuning parameter, leading less shrinkage. Increasing scale parameter also result less shrinkage. influence hyperparameters can visualized implemented shiny app, can called via shiny_prior().","code":"fit_lasso <- brma(yi ~ .,                   data = dat2l,                   vi = \"vi\",                   method = \"lasso\",                   prior = c(df = 1, scale = 1),                   mute_stan = FALSE)"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"assessing-convergence-and-interpreting-the-results","dir":"Articles","previous_headings":"Two-level model > Two-level model with the lasso prior","what":"Assessing convergence and interpreting the results","title":"Conducting a Bayesian Regularized Meta-analysis","text":"can request results using summary function. interpret results, need ensure MCMC chains converged posterior distribution. Two helpful diagnostics provided summary number effective posterior samples n_eff potential scale reduction factor Rhat. n_eff estimate number independent samples posterior. Ideally, ratio n_eff total samples close 1 possible. Rhat compares - within-chain estimates ideally close 1 (indicating chains mixed well). values n_eff Rhat far ideal values, can try increasing number iterations iter argument. default, brma function runs four MCMC chains 2000 iterations , half discarded burn-. result, total 4000 iterations available posterior summaries based. help, non-convergence might indicate problem model specification. satisfied convergence, can continue looking posterior summary statistics. summary function provides posterior mean estimate effect moderator. Since Bayesian penalization automatically shrink estimates exactly zero, additional criterion needed determine moderators selected model. Currently, done using 95% credible intervals, moderator selected zero excluded interval. summary denoted asterisk moderator. model, significant moderator dummy variable sex. coefficients significant shrunken towards zero lasso prior. Also note summary statistics tau2, (unexplained) residual -studies heterogeneity. 95% credible interval coefficient excludes zero, indicating non-zero amount unexplained heterogeneity. customary express heterogeneity terms I2I^2, percentage variation across studies due heterogeneity rather chance (Higgins Thompson, 2002; Higgins et al., 2003). helper function I2() computes posterior distribution I2I^2 based MCMC draws tau2:","code":"sum <- summary(fit_lasso) sum$coefficients[, c(\"mean\", \"sd\", \"2.5%\", \"97.5%\", \"n_eff\", \"Rhat\")] #>                                  mean      sd     2.5%   97.5% n_eff Rhat #> Intercept                    -20.7265 14.4136 -50.9609  3.6187  1657    1 #> mTimeLength                   -0.0032  0.0048  -0.0143  0.0049  2070    1 #> year                           0.0104  0.0072  -0.0017  0.0254  1660    1 #> modelLG                        0.0855  0.1438  -0.1759  0.4025  2283    1 #> modelLNB                       0.1430  0.1059  -0.0270  0.3721  1210    1 #> modelM                         0.0213  0.0496  -0.0695  0.1329  2208    1 #> modelMD                        0.0229  0.0773  -0.1253  0.1856  2320    1 #> ageWeek                       -0.0073  0.0053  -0.0187  0.0015  1364    1 #> strainGroupedC57Bl6           -0.0189  0.0662  -0.1587  0.1117  2193    1 #> strainGroupedCD1              -0.1158  0.1813  -0.5305  0.2004  2074    1 #> strainGroupedDBA              -0.0253  0.1625  -0.3563  0.3059  1591    1 #> strainGroupedlisterHooded     -0.0540  0.3135  -0.7133  0.5434  2062    1 #> strainGroupedlongEvans         0.0712  0.1205  -0.1421  0.3397  2253    1 #> strainGroupedlongEvansHooded   0.1790  0.2181  -0.1823  0.6557  2302    1 #> strainGroupedNMRI              0.1200  0.2423  -0.3336  0.6518  2225    1 #> strainGroupedNS                0.0289  0.1497  -0.2674  0.3448  2842    1 #> strainGroupedother            -0.0227  0.1263  -0.2832  0.2302  2079    1 #> strainGroupedspragueDawley    -0.0041  0.0615  -0.1343  0.1176  1921    1 #> strainGroupedswissWebster      0.5557  0.4900  -0.2266  1.6393  2230    1 #> strainGroupedwistar            0.0236  0.0567  -0.0807  0.1512  2078    1 #> strainGroupedwistarKyoto       0.0295  0.2698  -0.5350  0.5813  2392    1 #> bias                          -0.0086  0.0290  -0.0716  0.0473  2087    1 #> speciesrat                     0.1191  0.0877  -0.0203  0.3179  1380    1 #> domainsLearning                0.0017  0.0501  -0.1012  0.1072  1986    1 #> domainnsLearning               0.1159  0.0815  -0.0215  0.2892  1553    1 #> domainsocial                   0.1702  0.1183  -0.0249  0.4232  1600    1 #> domainnoMeta                  -0.3615  0.1843  -0.7299 -0.0184  1212    1 #> sexM                           0.1205  0.0689  -0.0017  0.2605  1602    1 #> tau2                           0.4291  0.0394   0.3524  0.5098  1112    1 I2(fit_lasso) #>    mean sd 2.5% 25% 50% 75% 97.5% #> I2   68  2   64  67  68  69    72"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"two-level-model-with-the-horseshoe-prior","dir":"Articles","previous_headings":"Two-level model","what":"Two-level model with the horseshoe prior","title":"Conducting a Bayesian Regularized Meta-analysis","text":"Next, look regularized horseshoe prior. horseshoe prior five hyperparameters can set. Three parameters degrees freedom parameters influence tails distributions prior. Generally, needed specify different values hyperparameters. , focus instead global scale parameter scale slab. Note horseshoe prior results divergent transitions. can indication non-convergence. However, divergences arise often using horseshoe prior long many , results can still used. Next, plot posterior mean estimates selection moderators different priors.  can see , general, different priors give quite similar results application. notable exception estimate dummy variable testAuthorGrouped_stepDownAvoidance much smaller lasso compared horseshoe specification. indicates lasso can shrink large coefficients towards zero whereas horseshoe better keeping large.","code":"# use the default settings fit_hs1 <- brma(yi ~ .,                 data = dat2l,                 vi = \"vi\",                 method = \"hs\",                 prior = c(df = 1, df_global = 1, df_slab = 4, scale_global = 1, scale_slab = 1, relevant_pars = NULL),                 mute_stan = FALSE)  # reduce the global scale fit_hs2 <- brma(yi ~ .,                 data = dat2l,                 vi = \"vi\",                 method = \"hs\",                 prior = c(df = 1, df_global = 1, df_slab = 4, scale_global = 0.1, scale_slab = 1, relevant_pars = NULL),                 mute_stan = FALSE)  # increase the scale of the slab fit_hs3 <- brma(yi ~ .,                 data = dat2l,                 vi = \"vi\",                 method = \"hs\",                 prior = c(df = 1, df_global = 1, df_slab = 4, scale_global = 1, scale_slab = 5, relevant_pars = NULL),                 mute_stan = FALSE) make_plotdat <- function(fit, prior){   plotdat <- data.frame(fit$coefficients)   plotdat$par <- rownames(plotdat)   plotdat$Prior <- prior   return(plotdat) }  df0 <- make_plotdat(fit_lasso, prior = \"lasso\") df1 <- make_plotdat(fit_hs1, prior = \"hs default\") df2 <- make_plotdat(fit_hs2, prior = \"hs reduced global scale\") df3 <- make_plotdat(fit_hs3, prior = \"hs increased slab scale\")  df <- rbind.data.frame(df0, df1, df2, df3) df <- df[!df$par %in% c(\"Intercept\", \"tau2\"), ] pd <- 0.5 ggplot(df, aes(x=mean, y=par, group = Prior)) +    geom_errorbar(aes(xmin=X2.5., xmax=X97.5., colour = Prior), width=.1, position = position_dodge(width = pd)) +   geom_point(aes(colour = Prior), position = position_dodge(width = pd)) +   geom_vline(xintercept = 0) +   theme_bw() + xlab(\"Posterior mean\") + ylab(\"\")"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"three-level-model","dir":"Articles","previous_headings":"","what":"Three-level model","title":"Conducting a Bayesian Regularized Meta-analysis","text":"Finally, can also take account fact effect sizes might come study fitting three-level model follows:","code":"fit_3l <- brma(yi ~ .,                data = datsel,                vi = \"vi\",                study = \"author\",                method = \"lasso\",                standardize = FALSE,                prior = c(df = 1, scale = 1),                mute_stan = FALSE)"},{"path":"https://cjvanlissa.github.io/pema/articles/using-brma.html","id":"standardization","dir":"Articles","previous_headings":"","what":"Standardization","title":"Conducting a Bayesian Regularized Meta-analysis","text":"possible override default standardization, standardizes variables (including dummies). , first manually standardize variables must standardized. , call brma(), provide named list elements list(center = ..., scale = ...). variables standardized, use center = 0, scale = 1. retains original scale. variables standardized, use original center scale restore coefficients original scale. example , standardize continuous predictor, standardize dummies: Note , example, ageWeek standardized; remaining (dummy) variables untouched. object stdz provides original center scale ageWeek, allows brma() properly rescale coefficient moderator. center scale remaining (dummy) variables 0 1, coefficients rescaled. See pema paper discussion standardization references.","code":"moderators <- model.matrix(yi~ageWeek + strainGrouped, data = datsel)[, -1] scale_age <- scale(moderators[,1]) stdz <- list(center = c(attr(scale_age, \"scaled:center\"), rep(0, length(levels(datsel$strainGrouped))-1)),              scale = c(attr(scale_age, \"scaled:scale\"),   rep(1, length(levels(datsel$strainGrouped))-1))) moderators <- data.frame(datsel[c(\"yi\", \"vi\", \"author\")], moderators) fit_std <- brma(yi ~ .,                 data = moderators,                 vi = \"vi\",                 study = \"author\",                 method = \"lasso\",                 prior = c(df = 1, scale = 1),                 standardize = stdz,                 mute_stan = FALSE)"},{"path":"https://cjvanlissa.github.io/pema/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Caspar J van Lissa. Author, maintainer. Sara J van Erp. Author.","code":""},{"path":"https://cjvanlissa.github.io/pema/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"C. J. V, S. V, E. B. C (2023). “Selecting relevant moderators using Bayesian regularized meta-regression.” Research Synthesis Methods. doi:10.31234/osf.io/6phs5, https://doi.org/10.31234/osf.io/6phs5.","code":"@Article{pema,   doi = {10.31234/osf.io/6phs5},   url = {https://doi.org/10.31234/osf.io/6phs5},   author = {Van Lissa {C. J.} and Van Erp S. and Clapper {E. B.}},   title = {Selecting relevant moderators using Bayesian regularized meta-regression},   journal = {Research Synthesis Methods},   year = {2023}, }"},{"path":"https://cjvanlissa.github.io/pema/index.html","id":"pema-penalized-meta-analysis-","dir":"","previous_headings":"","what":"Penalized Meta-Analysis","title":"Penalized Meta-Analysis","text":"Conduct penalized meta-analysis (“pema”) meta-analysis, often -study differences. can coded moderator variables, controlled using meta-regression. However, number moderators large relative number studies, analysis may overfitted. Penalized meta-regression useful cases, shrinks regression slopes irrelevant moderators towards zero.","code":""},{"path":"https://cjvanlissa.github.io/pema/index.html","id":"where-do-i-start","dir":"","previous_headings":"","what":"Where do I start?","title":"Penalized Meta-Analysis","text":"users, recommended starting point read paper published Research Synthesis Methods, introduces method, validates , provides tutorial example.","code":""},{"path":"https://cjvanlissa.github.io/pema/index.html","id":"installing-the-package","dir":"","previous_headings":"","what":"Installing the package","title":"Penalized Meta-Analysis","text":"Use CRAN install latest release pema: Alternatively, use R-universe install development version pema running following code:","code":"install.packages(\"pema\") options(repos = c(     cjvanlissa = 'https://cjvanlissa.r-universe.dev',     CRAN = 'https://cloud.r-project.org'))  install.packages('pema')"},{"path":"https://cjvanlissa.github.io/pema/index.html","id":"citing-pema","dir":"","previous_headings":"","what":"Citing pema","title":"Penalized Meta-Analysis","text":"can cite pema using following citation (please use citation either package, paper): Van Lissa, C. J., van Erp, S., & Clapper, E. B. (2023). Selecting relevant moderators Bayesian regularized meta-regression. Research Synthesis Methods. https://doi.org/10.31234/osf.io/6phs5","code":""},{"path":"https://cjvanlissa.github.io/pema/index.html","id":"about-this-repository","dir":"","previous_headings":"","what":"About this repository","title":"Penalized Meta-Analysis","text":"repository contains source code R-package called pema.","code":""},{"path":"https://cjvanlissa.github.io/pema/index.html","id":"contributing-and-contact-information","dir":"","previous_headings":"","what":"Contributing and Contact Information","title":"Penalized Meta-Analysis","text":"always eager receive user feedback contributions help us improve workflow software. Major contributions warrant coauthorship package. Please contact lead author c.j.vanlissa@uu.nl, : File GitHub issue feedback, bug reports feature requests Make pull request contribute code prose participating project, agree abide Contributor Code Conduct v2.0. Contributions package must adhere tidyverse style guide. contributing code, please add tests contribution tests/testthat folder, ensure tests pass GitHub Actions panel.","code":""},{"path":"https://cjvanlissa.github.io/pema/news.html","id":null,"dir":"","previous_headings":"","what":"pema 0.1.4","title":"pema 0.1.4","text":"Incorporate necessary changes maintain compatibility rstan Make Suggested packages truly optional, add test without Suggests Add tutorial vignette","code":""},{"path":"https://cjvanlissa.github.io/pema/news.html","id":"pema-013","dir":"","previous_headings":"","what":"pema 0.1.3","title":"pema 0.1.4","text":"Updated reference published validation paper Fixed bug .stan()","code":""},{"path":"https://cjvanlissa.github.io/pema/news.html","id":"pema-012","dir":"","previous_headings":"","what":"pema 0.1.2","title":"pema 0.1.4","text":"Updated maintainer email address Use str2lang(“pema::function”) instead quote(function) pema functions work without attaching package namespace Coerce mean sd array stan accepts cases 1 moderator Throw informative error cases 0 moderators","code":""},{"path":"https://cjvanlissa.github.io/pema/news.html","id":"pema-011","dir":"","previous_headings":"","what":"pema 0.1.1","title":"pema 0.1.4","text":"Refactor STAN code Support optional intercept Support three-level meta-analysis Add vignette Add Shiny app visualizing priors Add support multiple imputation","code":""},{"path":"https://cjvanlissa.github.io/pema/news.html","id":"pema-010","dir":"","previous_headings":"","what":"pema 0.1.0","title":"pema 0.1.4","text":"First submission CRAN","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/I2.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute I2 — I2","title":"Compute I2 — I2","text":"I2 represents amount heterogeneity relative total amount variance observed effect sizes (Higgins & Thompson, 2002). three-level meta-analyses, additionally broken I2_w (amount within-cluster heterogeneity) I2_b (amount -cluster heterogeneity).","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/I2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute I2 — I2","text":"","code":"I2(x, ...)"},{"path":"https://cjvanlissa.github.io/pema/reference/I2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute I2 — I2","text":"x object method exists. ... Arguments passed functions.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/I2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute I2 — I2","text":"Numeric matrix, rows corresponding I2 (total heterogeneity), optionally I2_w I2_b (within- -cluster heterogeneity).","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/I2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute I2 — I2","text":"","code":"I2(matrix(1:20, ncol = 1)) #>      mean      sd  2.5%  25%  50%   75%  97.5% #> [1,] 10.5 5.91608 1.475 5.75 10.5 15.25 19.525"},{"path":"https://cjvanlissa.github.io/pema/reference/as.stan.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an object to stanfit — as.stan","title":"Convert an object to stanfit — as.stan","text":"Create stanfit object object method exists, methods stanfit objects can used.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/as.stan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an object to stanfit — as.stan","text":"","code":"as.stan(x, ...)"},{"path":"https://cjvanlissa.github.io/pema/reference/as.stan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an object to stanfit — as.stan","text":"x object method exists. ... Arguments passed methods.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/as.stan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an object to stanfit — as.stan","text":"object class stanfit, documented rstan::stan.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/as.stan.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an object to stanfit — as.stan","text":"","code":"stanfit <- \"a\" class(stanfit) <- \"stanfit\" converted <- as.stan(stanfit)"},{"path":"https://cjvanlissa.github.io/pema/reference/bonapersona.html","id":null,"dir":"Reference","previous_headings":"","what":"Data from 'The behavioral phenotype of early life adversity' — bonapersona","title":"Data from 'The behavioral phenotype of early life adversity' — bonapersona","text":"meta-analysis rodent studies examined whether early life adversity (ELA) alters cognitive performance several domains. data include 400 independent experiments, involving approximately 8600 animals.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/bonapersona.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data from 'The behavioral phenotype of early life adversity' — bonapersona","text":"","code":"data(bonapersona)"},{"path":"https://cjvanlissa.github.io/pema/reference/bonapersona.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data from 'The behavioral phenotype of early life adversity' — bonapersona","text":"data.frame 734 rows 65 columns.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/bonapersona.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Data from 'The behavioral phenotype of early life adversity' — bonapersona","text":"Bonapersona, V., Kentrop, J., Van Lissa, C. J., van der Veen, R., Joels, M., & Sarabdjitsingh, R. . (2019). behavioral phenotype early life adversity: 3-level meta-analysis rodent studies. Neuroscience & Biobehavioral Reviews, 102, 299–307. doi:10.1016/j.neubiorev.2019.04.021","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/brma.html","id":null,"dir":"Reference","previous_headings":"","what":"Conduct Bayesian Regularized Meta-Analysis — brma","title":"Conduct Bayesian Regularized Meta-Analysis — brma","text":"function conducts Bayesian regularized meta-regression (Van Lissa & Van Erp, 2021). uses stan function rstan::sampling fit model. lasso horseshoe prior used shrink regression coefficients irrelevant moderators towards zero. See Details.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/brma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conduct Bayesian Regularized Meta-Analysis — brma","text":"","code":"brma(x, ...)  # S3 method for class 'formula' brma(   formula,   data,   vi = \"vi\",   study = NULL,   method = \"hs\",   standardize = TRUE,   prior = switch(method, lasso = c(df = 1, scale = 1), hs = c(df = 1, df_global = 1,     df_slab = 4, scale_global = 1, scale_slab = 2, relevant_pars = NULL)),   mute_stan = TRUE,   ... )  # Default S3 method brma(   x,   y,   vi,   study = NULL,   method = \"hs\",   standardize,   prior,   mute_stan = TRUE,   intercept,   ... )"},{"path":"https://cjvanlissa.github.io/pema/reference/brma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conduct Bayesian Regularized Meta-Analysis — brma","text":"x k x m numeric matrix, k number effect sizes m number moderators. ... Additional arguments passed rstan::sampling(). Use , e.g., override default arguments function. formula object class formula (one can coerced class), see lm. data Either data.frame containing variables model, see lm, list multiple imputed data.frames, object returned mice. vi Character. Name column data contains variances effect sizes. column removed data prior analysis. Defaults \"vi\". study Character. Name column data contains study id. Use data includes multiple effect sizes per study. column can vector integers, factor. column removed data prior analysis. See Details information analyzing dependent data. method Character, indicating type regularizing prior use. Supports one c(\"hs\", \"lasso\"), see Details. Defaults \"hs\". standardize Either logical argument list. standardize logical, controls whether predictors standardized prior analysis . Parameter estimates restored predictors' original scale. Alternatively, users can provide list standardize gain control standardization process. case, assumed standardization already taken place. list must two elements: list(center = c(mean(X1) , mean(X2), mean(X...)), scale = c(sd(X1), sd(X2), sd(X...))). used restore parameter estimates original scale predictors. useful, e.g., standardize continuous dichotomous variables separately. Defaults TRUE, recommended shrinking affects parameters similarly. prior Numeric vector, specifying prior use. Note different methods require vector contain specific named elements. mute_stan Logical, indicating whether mute 'Stan' output . y numeric vector k effect sizes. intercept Logical, indicating whether intercept included model.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/brma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conduct Bayesian Regularized Meta-Analysis — brma","text":"list object class brma, following structure:","code":"list(   fit          # An object of class stanfit, for compatibility with rstan   coefficients # A numeric matrix with parameter estimates; these are                # interpreted as regression coefficients, except tau2 and tau,                # which are interpreted as the residual variance and standard                # deviation, respectively.   formula      # The formula used to estimate the model   terms        # The predictor terms in the formula   X            # Numeric matrix of moderator variables   Y            # Numeric vector with effect sizes   vi           # Numeric vector with effect size variances   tau2         # Numeric, estimated tau2   R2           # Numeric, estimated heterogeneity explained by the moderators   k            # Numeric, number of effect sizes   study        # Numeric vector with study id numbers )"},{"path":"https://cjvanlissa.github.io/pema/reference/brma.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conduct Bayesian Regularized Meta-Analysis — brma","text":"Bayesian regularized meta-analysis algorithm (Van Lissa & Van Erp, 2021) penalizes meta-regression coefficients either via lasso prior (Park & Casella, 2008) regularized horseshoe prior (Piironen & Vehtari, 2017). lasso Bayesian equivalent lasso penalty obtained placing independent Laplace (.e., double exponential) priors regression coefficients centered around zero. scale Laplace priors determined global scale parameter scale, defaults 1 inverse-tuning parameter \\(\\frac{1}{\\lambda}\\) given chi-square prior governed degrees freedom parameter df (defaults 1). standardize = TRUE, shrinkage affect coefficients equally necessary adapt scale parameter. Increasing df parameter allow larger values inverse-tuning parameter, leading less shrinkage. hs One issue lasso prior relatively light tails. result, lasso desirable behavior pulling small coefficients zero, also results much shrinkage large coefficients. alternative prior improves upon shrinkage pattern horseshoe prior (Carvalho, Polson & Scott, 2010). horseshoe prior infinitely large spike zero, thereby pulling small coefficients toward zero addition fat tails, allow substantial coefficients escape shrinkage. regularized horseshoe extension horseshoe prior allows inclusion prior information regarding number relevant predictors can numerically stable certain cases (Piironen & Vehtari, 2017). regularized horseshoe global shrinkage parameter influences coefficients similarly local shrinkage parameters enable flexible shrinkage patterns coefficient separately. local shrinkage parameters given Student's t prior default df parameter 1. Larger values df result lighter tails prior longer strictly horseshoe prior. However, increasing df slightly might necessary avoid divergent transitions Stan (see also https://mc-stan.org/misc/warnings.html). Similarly, degrees freedom Student's t prior global shrinkage parameter df_global can increased default 1 , example, 3 divergent transitions occur although resulting prior strictly longer horseshoe. scale Student's t prior global shrinkage parameter scale_global defaults 1 can decreased achieve shrinkage. Moreover, prior information regarding number relevant moderators available, recommended include information via relevant_pars argument setting expected number relevant moderators. relevant_pars specified, scale_global ignored instead based available prior information. Contrary horseshoe prior, regularized horseshoe applies additional regularization large coefficients governed Student's t prior scale_slab defaulting 2 df_slab defaulting 4. additional regularization ensures least shrinkage large coefficients avoid sampling problems.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/brma.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conduct Bayesian Regularized Meta-Analysis — brma","text":"Van Lissa, C. J., van Erp, S., & Clapper, E. B. (2023). Selecting relevant moderators Bayesian regularized meta-regression. Research Synthesis Methods. doi:10.31234/osf.io/6phs5 Park, T., & Casella, G. (2008). Bayesian Lasso. Journal American Statistical Association, 103(482), 681–686. doi:10.1198/016214508000000337 Carvalho, C. M., Polson, N. G., & Scott, J. G. (2010). horseshoe estimator sparse signals. Biometrika, 97(2), 465–480. doi:10.1093/biomet/asq017 Piironen, J., & Vehtari, . (2017). Sparsity information regularization horseshoe shrinkage priors. Electronic Journal Statistics, 11(2). https://projecteuclid.org/journals/electronic-journal--statistics/volume-11/issue-2/Sparsity-information--regularization---horseshoe---shrinkage/10.1214/17-EJS1337SI.pdf","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/brma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conduct Bayesian Regularized Meta-Analysis — brma","text":"","code":"data(\"curry\") df <- curry[c(1:5, 50:55), c(\"d\", \"vi\", \"sex\", \"age\", \"donorcode\")] suppressWarnings({res <- brma(d~., data = df, iter = 10)})"},{"path":"https://cjvanlissa.github.io/pema/reference/check_workshop_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Data for BRMA Workshop — check_workshop_data","title":"Check Data for BRMA Workshop — check_workshop_data","text":"function checks argument df suitable data.frame completing workshop Bayesian Regularized Meta-Regression, checks pema package dependencies correctly installed. suggests remedy failed checks.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/check_workshop_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Data for BRMA Workshop — check_workshop_data","text":"","code":"check_workshop_data(df)"},{"path":"https://cjvanlissa.github.io/pema/reference/check_workshop_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Data for BRMA Workshop — check_workshop_data","text":"df data.frame.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/check_workshop_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Data for BRMA Workshop — check_workshop_data","text":"Invisibly returns logical TRUE/FALSE, indicating whether checks passed.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/check_workshop_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Data for BRMA Workshop — check_workshop_data","text":"","code":"check_workshop_data(fukkink_lont) #> Warning: restarting interrupted promise evaluation #> Warning: restarting interrupted promise evaluation"},{"path":"https://cjvanlissa.github.io/pema/reference/curry.html","id":null,"dir":"Reference","previous_headings":"","what":"Data from 'Happy to Help?' — curry","title":"Data from 'Happy to Help?' — curry","text":"systematic review meta-analysis effects performing acts kindness well-actor.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/curry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data from 'Happy to Help?' — curry","text":"","code":"data(curry)"},{"path":"https://cjvanlissa.github.io/pema/reference/curry.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data from 'Happy to Help?' — curry","text":"data.frame 56 rows 18 columns.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/curry.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Data from 'Happy to Help?' — curry","text":"Curry, O. S., Rowland, L. ., Van Lissa, C. J., Zlotowitz, S., McAlaney, J., & Whitehouse, H. (2018). Happy help? systematic review meta-analysis effects performing acts kindness well-actor. Journal Experimental Social Psychology, 76, 320-329. doi:10.1016/j.ecresq.2007.04.005","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/maxap.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximum a posteriori parameter estimate — maxap","title":"Maximum a posteriori parameter estimate — maxap","text":"Find parameter estimate highest posterior probability density given vector samples.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/maxap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximum a posteriori parameter estimate — maxap","text":"","code":"maxap(x, dens = NULL, ...)"},{"path":"https://cjvanlissa.github.io/pema/reference/maxap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximum a posteriori parameter estimate — maxap","text":"x Numeric vector. dens Optional object class density. Defaults NULL. ... Arguments passed density","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/maxap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximum a posteriori parameter estimate — maxap","text":"Atomic numeric vector maximum -posteriori estimate vector x.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/maxap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximum a posteriori parameter estimate — maxap","text":"","code":"maxap(c(1,2,3,4,5)) #> [1] 3.011742"},{"path":"https://cjvanlissa.github.io/pema/reference/pema-package.html","id":null,"dir":"Reference","previous_headings":"","what":"pema: Conduct penalized meta-regression. — pema-package","title":"pema: Conduct penalized meta-regression. — pema-package","text":"Penalized meta-regression shrinks regression slopes irrelevant moderators towards zero (Van Lissa & Van Erp, 2021).","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/pema-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"pema: Conduct penalized meta-regression. — pema-package","text":"Van Lissa, C. J., van Erp, S., & Clapper, E. B. (2023). Selecting relevant moderators Bayesian regularized meta-regression. Research Synthesis Methods. doi:10.31234/osf.io/6phs5 Stan Development Team (NA). RStan: R interface Stan. R package version 2.26.2. https://mc-stan.org","code":""},{"path":[]},{"path":"https://cjvanlissa.github.io/pema/reference/pema-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"pema: Conduct penalized meta-regression. — pema-package","text":"Maintainer: Caspar J van Lissa c.j.vanlissa@tilburguniversity.edu (ORCID) Authors: Sara J van Erp","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/plot_sensitivity.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot posterior distributions for BRMA models — plot_sensitivity","title":"Plot posterior distributions for BRMA models — plot_sensitivity","text":"perform rudimentary sensitivity analysis, plot posterior distributions multiple BRMA models compare visually.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/plot_sensitivity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot posterior distributions for BRMA models — plot_sensitivity","text":"","code":"plot_sensitivity(..., parameters = NULL, model_names = NULL)"},{"path":"https://cjvanlissa.github.io/pema/reference/plot_sensitivity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot posterior distributions for BRMA models — plot_sensitivity","text":"... Objects class brma. argument model_names NULL, names objects used label plot. parameters Optional character vector names parameters exist models ..., Default: NULL. model_names Optional character vector names used label models ..., Default: NULL","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/plot_sensitivity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot posterior distributions for BRMA models — plot_sensitivity","text":"object class ggplot","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/plot_sensitivity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot posterior distributions for BRMA models — plot_sensitivity","text":"","code":"plot_sensitivity(samples = list( data.frame(Parameter = \"b\", Value = rnorm(10), Model = \"M1\"), data.frame(Parameter = \"b\", Value = rnorm(10, mean = 2), Model = \"M2\")), parameters = \"b\")"},{"path":"https://cjvanlissa.github.io/pema/reference/sample_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from the Prior Distribution — sample_prior","title":"Sample from the Prior Distribution — sample_prior","text":"Samples prior distribution parameters defined prior. result can plotted using plot function.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/sample_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from the Prior Distribution — sample_prior","text":"","code":"sample_prior(   method = c(\"hs\", \"lasso\"),   prior = switch(method, lasso = c(df = 1, scale = 1), hs = c(df = 1, df_global = 1,     df_slab = 4, scale_global = 1, scale_slab = 2, par_ratio = NULL)),   iter = 1000 )"},{"path":"https://cjvanlissa.github.io/pema/reference/sample_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from the Prior Distribution — sample_prior","text":"method Character string, indicating prior sample . Default: first element c(\"hs\", \"lasso\"). prior Numeric vector, specifying prior use. See brma details. iter positive integer specifying number iterations sample. Default: 1000","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/sample_prior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from the Prior Distribution — sample_prior","text":"NULL, function called side-effect plotting graphics device.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/sample_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from the Prior Distribution — sample_prior","text":"","code":"sample_prior(\"lasso\", iter = 10) #> $method #> [1] \"lasso\" #>  #> $iter #> [1] 10 #>  #> $samples #> Inference for Stan model: lasso_prior. #> 1 chains, each with iter=10; warmup=0; thin=1;  #> post-warmup draws per chain=10, total post-warmup draws=10. #>  #>                   mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat #> b                -0.26    0.66 1.71 -2.91 -1.23  0.05  1.18  1.72     7 0.89 #> lasso_inv_lambda  1.80    0.50 1.59  0.36  0.86  0.86  2.49  4.60    10 0.90 #> lp__             -4.04    0.22 0.71 -5.12 -4.57 -3.90 -3.41 -3.32    10 0.99 #>  #> Samples were drawn using NUTS(diag_e) at Sat Mar 29 16:35:06 2025. #> For each parameter, n_eff is a crude measure of effective sample size, #> and Rhat is the potential scale reduction factor on split chains (at  #> convergence, Rhat=1). #>  #> attr(,\"class\") #> [1] \"brma_prior\" \"list\""},{"path":"https://cjvanlissa.github.io/pema/reference/shiny_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Interactively Sample from the Prior Distribution — shiny_prior","title":"Interactively Sample from the Prior Distribution — shiny_prior","text":"Launches Shiny app allows interactive comparison different priors brma.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/shiny_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interactively Sample from the Prior Distribution — shiny_prior","text":"","code":"shiny_prior()"},{"path":"https://cjvanlissa.github.io/pema/reference/shiny_prior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interactively Sample from the Prior Distribution — shiny_prior","text":"NULL, function called side-effect launching Shiny app.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/shiny_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interactively Sample from the Prior Distribution — shiny_prior","text":"","code":"if (FALSE) { # \\dontrun{ shiny_prior() } # }"},{"path":"https://cjvanlissa.github.io/pema/reference/simulate_smd.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulates a meta-analytic dataset — simulate_smd","title":"Simulates a meta-analytic dataset — simulate_smd","text":"function simulates meta-analytic dataset based random-effects model. simulated effect size Hedges' G, estimator Standardized Mean Difference (Hedges, 1981; Li, Dusseldorp, & Meulman, 2017). functional form model can specified, moderators can either normally distributed Bernoulli-distributed. See Van Lissa, preparation, detailed explanation simulation procedure.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/simulate_smd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulates a meta-analytic dataset — simulate_smd","text":"","code":"simulate_smd(   k_train = 20,   k_test = 100,   mean_n = 40,   es = 0.5,   tau2 = 0.04,   alpha = 0,   moderators = 5,   distribution = \"normal\",   model = \"es * x[, 1]\" )"},{"path":"https://cjvanlissa.github.io/pema/reference/simulate_smd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulates a meta-analytic dataset — simulate_smd","text":"k_train Atomic integer. number studies training dataset. Defaults 20. k_test Atomic integer. number studies testing dataset. Defaults 100. mean_n Atomic integer. mean sample size simulated study meta-analytic dataset. Defaults 40. simulated study, sample size n randomly drawn normal distribution mean mean_n, sd mean_n/3. es Atomic numeric vector. effect size, also known beta, used model statement. Defaults .5. tau2 Atomic numeric vector. residual heterogeneity. range realistic values encountered psychological research, see Van Erp, Verhagen, Grasman, & Wagenmakers, 2017. Defaults 0.04. alpha Vector slant parameters, passed sn::rsn. moderators Atomic integer. number moderators simulate study. Make sure number moderators simulated least large number moderators referred model parameter. Internally, matrix moderators referred \"x\". Defaults 5. distribution Atomic character. distribution moderators. Can set either \"normal\" \"bernoulli\". Defaults \"normal\". model Expression. expression specify model simulate mean true effect size, mu. formula may use terms \"es\" (referring es parameter call simulate_smd), \"x\\[, \\]\" (referring matrix moderators, x). Thus, specify mean effect size, mu, function effect size first moderator, one pass value model = \"es * x\\[ , 1\\]\". Defaults \"es * x\\[ , 1\\]\".","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/simulate_smd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulates a meta-analytic dataset — simulate_smd","text":"List length 4. \"training\" element list data.frame k_train rows. columns variance effect size, vi; effect size, yi, moderators, X. \"testing\" element list data.frame k_test rows. columns effect size, yi, moderators, X. \"housekeeping\" element list data.frame k_train + k_test rows. columns n, sample size n simulated study; mu_i, mean true effect size simulated study; theta_i, true effect size simulated study.","code":""},{"path":"https://cjvanlissa.github.io/pema/reference/simulate_smd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulates a meta-analytic dataset — simulate_smd","text":"","code":"set.seed(8) simulate_smd() #> $training #>            vi          yi         X1          X2           X3         X4 #> 1  0.10123023 -0.16012845 -1.1444094 -1.04043881  0.062535228  1.0148301 #> 2  0.08088794 -0.80729991 -1.7215353  0.02465913  0.231113964 -0.8321755 #> 3  0.11803209  0.63219536 -0.3696855  0.86220033  1.517608501 -1.4727851 #> 4  0.12802095  0.76916686  1.8220755  0.70828681 -0.498742894  0.4537334 #> 5  0.08189427  0.66239160  0.4775898 -0.63656810  0.004252849  0.6631271 #> 6  0.10243131 -0.34194013  0.1405485  0.90875200  0.359516304 -0.7153895 #> 7  0.10480936 -0.54557741 -1.7260245 -0.58273245  2.082965890  2.0207644 #> 8  0.14436434  0.06504549  0.2765317  0.08109775 -1.408034082  1.9224919 #> 9  0.38418205  0.31267138 -1.3627597 -0.04364549  1.217850494 -0.5379136 #> 10 0.12307917  0.52473228  0.8354238  0.27538614  0.721791139 -1.1210611 #> 11 0.15875395  1.39707761  2.2256232  1.03476676 -0.201646518 -0.7519621 #> 12 0.08878501  0.31292809  0.1722670 -0.44401335  0.095368307 -0.4055921 #> 13 0.10208681 -1.28993059 -1.5005928 -1.34238940 -2.032627280 -0.1743360 #> 14 0.17384976 -0.49542668 -1.3466174 -0.16337964 -1.775771585  0.9972979 #> 15 0.10038187 -0.58759913 -2.4010823 -0.24919870  0.066025885  0.5076123 #> 16 0.14694100  1.11492088  0.7935204  0.66247074 -1.224103992 -0.9695288 #> 17 0.06776487  0.58157303  0.4438913  0.64397076  1.578454567  0.2136013 #> 18 0.11094361  0.58270488  0.5038261 -0.85192796 -0.204568006 -0.6238114 #> 19 0.07510350  1.10402148  1.4280847 -0.72472537  0.413325429  0.3302807 #> 20 0.12887103 -1.27683963 -1.8320869 -0.13801556 -0.355544375 -1.3008866 #>             X5 #> 1   0.88819728 #> 2   0.29270972 #> 3   0.31683564 #> 4  -2.02425302 #> 5  -0.91530048 #> 6  -0.34304121 #> 7  -0.61830454 #> 8   1.96917331 #> 9   1.28234666 #> 10  1.21800036 #> 11 -1.27702516 #> 12  2.08310267 #> 13 -0.65815868 #> 14 -0.34382308 #> 15  0.74423183 #> 16 -1.32233177 #> 17  0.39138003 #> 18 -0.09488615 #> 19  1.13265525 #> 20  0.46624137 #>  #> $testing #>                yi          X1           X2          X3          X4           X5 #> 21   0.0697442490  1.34278280 -0.345707310  1.57818995 -0.44913981  0.705276551 #> 22   0.6610788203  2.43894434  0.407881830 -1.09289061  0.02348950 -1.346451232 #> 23   0.9563700453  0.67693611 -0.542383594 -0.76616169  0.32860881  1.606175428 #> 24  -0.4565151859 -1.96898063 -2.233980278 -2.44080992 -0.65851847  2.978035548 #> 25   0.2097207276  0.08038142  1.317620135 -0.66787498 -0.69390119  0.889280799 #> 26  -0.0699897413 -0.70476923  0.421883928 -0.87779243 -1.02011480 -0.364122280 #> 27   0.1125065157  0.59725232 -0.742518989 -0.24948742 -1.10058841  0.054619107 #> 28   0.5799963557  0.79577963  0.246548388 -0.91454381 -1.61812166 -0.589375171 #> 29  -0.4113890204 -1.03083607 -0.534765887  0.08087580  2.14679976  0.050225736 #> 30  -0.1466466750 -0.41952683 -1.375543934 -0.08288448 -0.71783589  2.617582843 #> 31   0.2641604408  0.16009113 -0.410783842 -0.40642879  0.89826920  0.627148220 #> 32   0.5488677329 -0.56448033  1.072109345  0.69900714 -1.15007197 -0.827227458 #> 33  -0.1410646978  0.22750654  0.944193458  0.47244670 -1.04069937  0.504060258 #> 34  -0.1047712893  0.25707888 -0.660594907  0.98195504 -0.06291613 -0.394589512 #> 35   0.5656267024 -0.33770080  0.463225218 -1.44548947  1.37773115  0.357938059 #> 36   0.3871240265  0.30457246 -0.833497207  0.01650441  0.62319204 -0.115376387 #> 37   0.2289248513  0.72443709 -1.656108832 -0.19116183 -0.69531851  0.680534148 #> 38  -0.7671912986 -0.68517330 -0.242947018  0.60615056  0.34736006  0.377851898 #> 39   0.8280179901  0.66148590 -0.734829314  1.11059186 -1.66093486  0.386261629 #> 40   1.7946267727  1.72541962  0.288050068 -0.02910979  1.22541007  0.549553058 #> 41   0.1113967154  0.32134570 -2.519865202 -0.29233821  1.04911272  0.613320135 #> 42   0.0488309692  0.57631589  0.559240717  1.47869193  0.07042518 -0.795713800 #> 43  -0.6429384837 -1.40999218 -0.370775118  0.73176907  0.34117199 -1.490813982 #> 44  -0.0108968411 -0.99550207 -0.246350631 -1.19898977 -0.45694682 -0.769902613 #> 45   1.1389625045  1.56029895 -1.068533578  1.51577158  0.36241300  1.625447491 #> 46   0.0464288770  0.20359400  0.188859634 -0.36821179  0.43088979  0.903666849 #> 47  -0.7271016178 -0.38763480 -0.700101046 -0.61153291 -3.28193174 -0.938239707 #> 48   0.3385902120  0.91961483 -0.054871878 -0.99407239  0.07959148 -0.863328748 #> 49  -0.3364394483 -0.37973893  0.004724072  0.22384503  0.28338913 -0.012054345 #> 50   0.0721290798  0.74424525  0.513744294  1.24460620 -1.13800059 -1.018069491 #> 51  -0.5792457661 -1.18318210 -0.232082803 -2.14666505  0.55833481 -1.304871448 #> 52   0.5368899917  0.27641906 -0.503803381  0.52658047  1.45478371 -0.907842475 #> 53   0.0869024855 -0.35621772  0.931147030 -0.13259284  0.91997026  0.045246555 #> 54   0.0793080670  0.98772386  0.866682884 -0.36330720 -2.45861485  1.591761639 #> 55  -0.4977438187 -0.26723365 -1.615076668 -0.89040754  1.35749161  0.146493228 #> 56   0.4946680032  0.48002729 -0.414854751 -0.54416473 -0.73312796 -0.463292606 #> 57   0.8300930074  0.76742274  0.295879429 -0.46053733  0.38481593  0.236661956 #> 58   0.4046279101  0.93944209 -0.501413917 -0.19810448  0.92773032  0.664210399 #> 59  -0.9582677316 -1.05321259  0.168396365 -0.05398731 -1.23938949  0.629995102 #> 60   1.0929122623  1.01361777 -1.484505096  0.06713632  1.73655423 -0.219841525 #> 61  -1.2446328798 -1.25417324 -0.935164736  1.37062115  0.46932808  0.001456405 #> 62  -0.0447009949 -0.38058363 -1.318323265 -1.71261554  0.27094446 -0.450508307 #> 63  -0.9829585596  0.20908883 -0.843522645 -1.54077184 -0.66946378 -0.418097081 #> 64  -0.2109305739 -0.12475618 -0.180237064  1.42855685  0.54643929  0.338293573 #> 65   1.2770131383  2.18637793 -1.473517194 -1.19629850  0.29537750 -0.393182816 #> 66  -0.6904278987  0.08202324  0.296855110  0.88030339 -1.13881229 -0.354461946 #> 67   0.9332104802  2.11210205  0.483606325 -1.68208244  1.29022620 -1.206672296 #> 68   0.4295536525  1.03434270  0.068466459  1.69888450 -2.22984375 -0.412220453 #> 69  -0.5541784759 -0.19817321  0.288832146 -0.15496161  1.28430248  0.386628349 #> 70   0.1773691167 -1.03922958  1.314781376 -1.27400097 -1.41318801 -0.619818844 #> 71  -1.1119527377 -0.13366410 -0.295572687 -0.58194548 -0.69842091  1.081208993 #> 72  -0.2327487049 -0.58203619 -0.064578436 -0.43066068  0.82325010 -1.678908961 #> 73  -0.6881029992 -0.90301123  0.776282190  0.30444699 -1.14748923 -1.440690533 #> 74   0.2872110208 -0.80183175 -1.079610620  0.33526170  0.02986996 -1.422982215 #> 75   0.9580626631  1.32082067  0.592149208 -2.28425822 -0.05187127 -1.138380472 #> 76   0.6505781727  1.03917965  0.326762664 -0.87895358 -0.29070076 -1.560017465 #> 77   0.6396433924  0.55811104 -2.295856107  0.98105921 -0.06218890 -0.468259305 #> 78  -0.4669543062 -1.32868715 -0.402928719  0.74271560 -1.07172843  0.467356785 #> 79  -0.9769101988 -1.92455679  1.125903449  0.10056759 -1.16839102 -0.146917636 #> 80  -0.2932424292  0.02269541  1.063092737  0.16902021 -0.19654398  1.624582143 #> 81  -0.1995739488 -0.58148841 -1.156393348  0.34330655 -1.23256601  0.424045409 #> 82   0.2074836427 -0.22415163 -0.954671006  1.03191632  1.67582582 -0.560888865 #> 83   1.0590227386  1.19843419 -0.642356883  1.61580763  0.14981857 -0.229831775 #> 84  -0.9464700091 -1.35857871 -0.129691509 -2.05943539  1.10504698  0.994997471 #> 85   0.0501360937 -0.02007139  0.295829171  0.71146123 -0.16625956 -0.669661078 #> 86   0.7211893129  0.89013344  0.824899518 -0.19750984 -1.62326723  0.071204845 #> 87  -0.0192254232 -0.02785890 -1.148831250 -1.55998272  0.54408680 -1.183813504 #> 88  -0.2100014832 -0.87933866  1.585573941  0.39203385  0.43202330 -0.976633307 #> 89   0.5415433122  0.76532204 -0.132846093 -0.10443202 -1.73795791  1.869923666 #> 90  -0.3031373341 -0.97758855 -0.118899731 -0.30398595  1.32981739 -0.575898320 #> 91  -0.2433747657  0.53030495  0.119426695 -0.37146423 -0.82576253 -1.019094922 #> 92   0.9812211327  0.93490598  0.193494092  1.28880895 -0.74782165 -0.649659900 #> 93   0.0312254970  0.34618828  1.469472441  0.16832416 -2.13353795  1.533933071 #> 94  -0.4437473468 -0.73358085 -2.160397828  0.79351401  0.36263912 -0.107787116 #> 95  -0.1699726514  0.71044321  2.309691146 -1.43665381 -0.23749230  0.169535331 #> 96  -0.6784711467 -0.32270804  0.984586572 -2.80920276 -0.91259245  1.008258732 #> 97   0.0008929241 -0.36063854  1.510773654  0.24555120 -0.54637269  1.177163008 #> 98   0.3171494928  0.15515330 -0.044507754 -0.42755197  0.17847832  0.837676659 #> 99  -0.3849614859 -1.39090615 -1.153481015  0.32787243 -0.99193808  0.669284255 #> 100  0.7591054389  1.37319401  2.294143138  0.24435018 -0.86142982 -0.243659774 #> 101 -0.5768676823 -0.86514969  0.538671399  0.94024364  0.45566891  1.129335915 #> 102 -1.1473919063 -1.30250885 -0.146357462 -0.13692522 -2.05915568 -0.041590377 #> 103  0.7351544763  0.89342076 -0.979218618 -1.85424344  0.16398290  0.914060323 #> 104 -0.8653556107 -0.88037666 -1.831172832  1.64373845  0.78839628  0.432991085 #> 105  0.0245160987 -0.41661945  1.870035998 -0.14302351 -1.09377744 -0.936613809 #> 106  0.3512162331 -0.04062916 -1.503020392 -0.52419166 -0.53807090 -0.582746486 #> 107  0.1455497552  0.14358965  2.318318542  0.78931815 -0.22432943 -0.090653685 #> 108  0.0719350527  0.09751138 -1.256282212  0.96144097  1.46733421  1.957436095 #> 109  0.2745557083  0.16642969 -0.938503513  1.29350891 -1.60605679  0.843387407 #> 110  1.0883455067  1.47631110 -0.526780039 -1.35604594 -0.54841970  0.115736923 #> 111 -1.5142330943 -1.54600953 -1.733278053 -0.85199073 -1.12518922  0.051878985 #> 112  0.5818380085  1.37092704 -1.078728380 -1.10291173  0.37742996 -0.559855595 #> 113  0.3096846021 -0.30687696 -0.369739572  0.84291478 -0.35847353  0.039211626 #> 114 -0.3923435819 -0.17814964  1.418624142 -0.40487444  0.02549246  0.573795451 #> 115  0.7116822116  0.84086555 -0.960515383  0.34463034 -0.83657908 -1.722671808 #> 116 -0.2505867019 -0.70385466  0.444572597  0.11589273  0.34432991  1.525124902 #> 117 -0.7169171557 -0.38034853  0.920231427  0.02267681  1.26276866 -0.035198961 #> 118 -0.0128238583 -0.61284745  0.248415976  1.81038765  0.32824879 -0.187316677 #> 119  0.7318241641 -0.84761010 -0.258117777  0.18494050 -0.03846550  0.602229762 #> 120  1.0879095578  1.45125747  2.300613151 -0.42790075  0.55285433  0.401261034 #>  #> $housekeeping #>      n        mu_i      theta_i #> 1   38 -0.57220472 -0.705987796 #> 2   52 -0.86076766 -0.698131590 #> 3   34 -0.18484276  0.040520862 #> 4   32  0.91103776  0.737997061 #> 5   50  0.23879490  0.339999953 #> 6   38  0.07027426  0.095919189 #> 7   38 -0.86301223 -0.878455945 #> 8   26  0.13826583  0.473640449 #> 9    8 -0.68137984 -0.498834172 #> 10  32  0.41771190  0.574197305 #> 11  30  1.11281162  1.334479699 #> 12  44  0.08613348  0.187929740 #> 13  46 -0.75029639 -0.764245280 #> 14  22 -0.67330868 -0.720849180 #> 15  40 -1.20054114 -0.965045888 #> 16  30  0.39676020  0.489142940 #> 17  60  0.22194567  0.447451857 #> 18  36  0.25191305  0.072457827 #> 19  60  0.71404233  0.894284264 #> 20  36 -0.91604347 -1.271038423 #> 21  58  0.67139140  0.335185599 #> 22  40  1.21947217  0.989581609 #> 23  34  0.33846806  0.403643679 #> 24  40 -0.98449031 -0.564007419 #> 25  64  0.04019071  0.170906391 #> 26  26 -0.35238462 -0.077853808 #> 27  26  0.29862616  0.117139104 #> 28  66  0.39788982  0.752628158 #> 29  48 -0.51541804 -0.610055705 #> 30  14 -0.20976341 -0.655667187 #> 31  60  0.08004557  0.183319624 #> 32  52 -0.28224017 -0.214589007 #> 33  20  0.11375327 -0.229213055 #> 34  30  0.12853944  0.019967531 #> 35  56 -0.16885040 -0.060407138 #> 36  46  0.15228623  0.177478933 #> 37  52  0.36221854  0.294360825 #> 38  30 -0.34258665 -0.255392535 #> 39  40  0.33074295  0.446835494 #> 40  46  0.86270981  1.136663478 #> 41  44  0.16067285  0.281965264 #> 42  32  0.28815795  0.421205467 #> 43  44 -0.70499609 -0.520115340 #> 44  36 -0.49775103 -0.413206121 #> 45  48  0.78014947  0.729160272 #> 46  14  0.10179700  0.141820091 #> 47  32 -0.19381740 -0.832247539 #> 48  44  0.45980742  0.602377723 #> 49  44 -0.18986946 -0.463477153 #> 50  32  0.37212263  0.238516369 #> 51  54 -0.59159105 -0.651683154 #> 52  36  0.13820953  0.236078495 #> 53  48 -0.17810886 -0.303963048 #> 54  38  0.49386193  0.091643728 #> 55  52 -0.13361682 -0.319818014 #> 56  48  0.24001364  0.545196640 #> 57  30  0.38371137  0.265529533 #> 58  32  0.46972104  0.274561431 #> 59  20 -0.52660630 -0.234668518 #> 60  50  0.50680889  0.596736262 #> 61  48 -0.62708662 -0.687148534 #> 62  42 -0.19029182 -0.001593116 #> 63  16  0.10454441 -0.033059907 #> 64  26 -0.06237809 -0.001719045 #> 65  36  1.09318897  1.388980572 #> 66  26  0.04101162 -0.312696335 #> 67  34  1.05605102  0.759007683 #> 68  62  0.51717135  0.575608950 #> 69  46 -0.09908660 -0.027193541 #> 70  18 -0.51961479 -0.693083484 #> 71  20 -0.06683205 -0.329647608 #> 72  54 -0.29101810 -0.368656232 #> 73  50 -0.45150561 -0.354471888 #> 74  34 -0.40091587 -0.057682519 #> 75  42  0.66041034  0.977170894 #> 76  48  0.51958982  0.658157047 #> 77   8  0.27905552  0.265934113 #> 78  48 -0.66434357 -0.351926634 #> 79  72 -0.96227839 -0.938916485 #> 80  32  0.01134770 -0.051035547 #> 81  44 -0.29074421 -0.697014238 #> 82  30 -0.11207581 -0.029428910 #> 83  26  0.59921709  0.676334065 #> 84  68 -0.67928936 -0.787838095 #> 85  18 -0.01003569 -0.224804131 #> 86  56  0.44506672  0.608669640 #> 87  38 -0.01392945 -0.154583120 #> 88  28 -0.43966933 -0.262234146 #> 89  48  0.38266102  0.219259279 #> 90   8 -0.48879427 -0.612113070 #> 91  50  0.26515247  0.173957761 #> 92  60  0.46745299  0.660231719 #> 93  34  0.17309414  0.544501930 #> 94  32 -0.36679042 -0.179546808 #> 95  22  0.35522160  0.490301625 #> 96  50 -0.16135402 -0.179158919 #> 97  44 -0.18031927 -0.198168992 #> 98  40  0.07757665 -0.104464305 #> 99  32 -0.69545307 -0.536135214 #> 100 40  0.68659701  0.997765972 #> 101 44 -0.43257484 -0.469403356 #> 102 14 -0.65125442 -1.002375278 #> 103 18  0.44671038  0.558136376 #> 104 16 -0.44018833 -0.552172900 #> 105 40 -0.20830972 -0.036721502 #> 106 34 -0.02031458  0.003837812 #> 107 48  0.07179483  0.125910961 #> 108 36  0.04875569  0.074342221 #> 109 42  0.08321484  0.125550152 #> 110 48  0.73815555  0.573441346 #> 111 38 -0.77300477 -0.945305162 #> 112 28  0.68546352  0.998741107 #> 113 48 -0.15343848 -0.152354218 #> 114 26 -0.08907482 -0.524721658 #> 115 60  0.42043278  0.865017910 #> 116 58 -0.35192733 -0.467546114 #> 117 44 -0.19017427 -0.417262393 #> 118 60 -0.30642373 -0.517382143 #> 119 24 -0.42380505 -0.059420545 #> 120 36  0.72562874  1.012973300 #>  #> $tau2_est #> [1] 0.5012652 #>  simulate_smd(k_train = 50, distribution = \"bernoulli\") #> $training #>            vi          yi X1 X2 X3 X4 X5 #> 1  0.08942930  0.39321957  1  0  1  0  0 #> 2  0.12811877 -0.33721651  0  0  0  0  1 #> 3  0.10706848  0.24604020  1  0  1  0  0 #> 4  0.09230692  0.22982552  0  1  0  0  1 #> 5  0.38332496  0.28991385  1  0  0  0  0 #> 6  0.08071917  0.09584201  0  0  1  0  1 #> 7  0.07765022 -0.11982391  1  1  1  0  0 #> 8  0.08410053  0.09583085  1  1  0  1  1 #> 9  0.07567228  0.33061352  1  1  1  1  1 #> 10 0.05974381 -0.27076113  0  1  0  0  1 #> 11 0.08200838  0.67094921  1  1  1  1  0 #> 12 0.09189174 -0.13395876  1  1  1  1  0 #> 13 0.09168652 -0.02657789  0  1  0  1  0 #> 14 0.10949411  0.48495426  1  1  1  0  1 #> 15 0.17029310  0.84633347  1  0  1  1  0 #> 16 0.12764148  0.29168029  0  0  1  0  0 #> 17 0.09285864  0.31490347  1  1  0  0  0 #> 18 0.22939280  0.43531063  0  1  1  0  0 #> 19 0.06495430 -0.03096112  0  0  1  1  1 #> 20 0.10637747  1.28298894  1  1  0  0  1 #> 21 0.09028827 -0.47980299  0  0  1  1  0 #> 22 0.10448141 -0.82050940  0  0  1  0  0 #> 23 0.20296312  0.22458034  1  0  1  1  1 #> 24 0.11217933  0.04102836  1  1  0  0  0 #> 25 0.07033191  0.31444003  1  1  0  0  0 #> 26 0.11898230 -0.11464852  0  0  0  1  1 #> 27 0.09034614  1.40981795  1  1  0  0  0 #> 28 0.07466851 -0.07008951  1  1  0  0  1 #> 29 0.08476435  0.85192169  1  1  1  1  0 #> 30 0.06957263  0.11761361  0  0  1  0  0 #> 31 0.09200744  0.16632550  1  1  1  0  0 #> 32 0.23375408  1.07652726  1  1  0  1  0 #> 33 0.08920365  0.36710309  1  1  1  0  1 #> 34 0.20875650  0.50891856  1  1  0  0  1 #> 35 0.09845814  0.43746378  1  0  1  1  0 #> 36 0.08012978 -0.51216548  0  1  1  0  0 #> 37 0.10101764 -0.09738429  0  1  1  0  0 #> 38 0.10631629  0.07986114  0  0  0  1  1 #> 39 0.11530796  0.46306563  0  0  0  1  0 #> 40 0.06733517  0.53540920  1  1  1  0  0 #> 41 0.12459935  0.91991545  1  0  1  0  1 #> 42 0.04837183  0.28764929  0  1  1  0  0 #> 43 0.06760571  0.23677321  1  1  1  1  1 #> 44 0.10118482 -0.14896282  0  1  0  0  0 #> 45 0.07235750  0.57073451  1  0  1  0  1 #> 46 0.12754742 -0.28184082  0  0  0  1  0 #> 47 0.09899778 -0.48429881  0  1  1  1  0 #> 48 0.10535158  0.58211960  0  1  1  0  1 #> 49 0.16895308  0.17318762  1  0  1  1  0 #> 50 0.09225451 -0.22004001  0  0  0  0  1 #>  #> $testing #>                yi X1 X2 X3 X4 X5 #> 51   1.3610589057  1  0  1  0  1 #> 52   0.9905762093  1  1  1  1  1 #> 53  -0.5663433441  0  1  0  0  0 #> 54   0.0325784399  0  1  0  1  0 #> 55  -0.3987294694  0  0  0  0  0 #> 56   1.4394713431  1  1  0  1  0 #> 57   0.8641291202  1  1  0  0  0 #> 58  -0.4664070750  0  1  1  0  0 #> 59  -0.2271211308  0  1  0  0  1 #> 60   0.4873625931  0  1  1  0  0 #> 61   1.0251315589  1  1  1  1  1 #> 62   0.2576578399  1  1  0  1  1 #> 63  -0.1346760556  1  1  1  1  1 #> 64   0.2827026925  0  1  0  0  0 #> 65   0.1992093839  1  0  0  0  0 #> 66   1.4388694026  1  0  1  0  0 #> 67   1.0564717221  0  1  1  0  0 #> 68  -0.0698590974  0  1  1  1  1 #> 69  -0.4445950100  0  0  1  1  0 #> 70   0.7217065925  1  1  0  0  1 #> 71   0.3877110174  1  1  1  1  1 #> 72   0.7805664984  1  1  1  0  1 #> 73  -0.0028772005  1  0  0  1  1 #> 74   1.1211745574  1  0  1  1  1 #> 75   0.3766041939  1  0  0  0  0 #> 76   0.1909042313  0  0  1  1  0 #> 77   0.4634482049  1  1  1  1  0 #> 78   0.1673482322  1  0  0  1  1 #> 79   0.5069101185  0  0  0  1  0 #> 80   0.5934150967  1  1  1  0  0 #> 81   0.2657470211  0  0  1  0  1 #> 82  -0.8205912639  0  1  0  1  1 #> 83   0.2563534563  0  0  0  1  1 #> 84  -0.3914667582  0  1  1  0  1 #> 85   0.7507259369  1  0  1  0  0 #> 86   0.6421525361  0  0  0  1  0 #> 87   0.0385397449  0  0  0  0  1 #> 88   0.4260140167  1  0  1  1  0 #> 89   0.5677753989  1  1  1  1  0 #> 90   0.3754579113  0  0  1  0  1 #> 91   0.0480205794  0  0  1  1  0 #> 92   0.3682180208  0  0  0  0  1 #> 93   0.5023048015  0  0  0  1  1 #> 94   0.2122716736  1  0  0  0  1 #> 95   1.1781509262  1  0  0  0  0 #> 96   0.1483738906  0  1  1  1  0 #> 97  -0.4509965470  0  1  0  0  0 #> 98   0.7176483438  1  0  0  1  0 #> 99  -0.1933798945  0  0  1  0  1 #> 100  0.6545642916  1  0  1  0  0 #> 101  0.7374296942  1  0  1  1  0 #> 102  1.0594813192  1  1  0  1  1 #> 103  0.0912794627  0  0  0  0  1 #> 104 -0.4199702552  0  0  1  0  0 #> 105  0.2211928770  1  1  0  0  1 #> 106  0.3246312380  1  1  1  1  1 #> 107 -0.0611561721  1  0  0  0  1 #> 108  0.3621610330  0  1  1  0  1 #> 109  0.2245841914  1  1  1  1  0 #> 110 -0.3058386101  0  1  0  1  0 #> 111  0.4660112449  1  1  0  1  0 #> 112  0.3991601309  1  1  0  0  1 #> 113 -0.1605873821  0  1  1  1  1 #> 114 -0.5967452625  1  1  0  1  1 #> 115  0.7097746996  1  0  0  1  0 #> 116 -0.2909741799  0  0  0  1  0 #> 117  0.0046824741  0  1  0  0  0 #> 118  0.1028707296  0  0  1  0  1 #> 119  0.1224422551  1  0  1  1  1 #> 120  0.3166730050  1  1  0  0  1 #> 121  0.1847810047  1  1  0  0  0 #> 122  0.4427034907  1  0  0  1  1 #> 123  0.1658383133  0  0  0  1  1 #> 124  0.2188132119  0  1  1  1  0 #> 125  1.0097088477  1  0  0  0  0 #> 126  1.3221468005  1  0  1  0  0 #> 127  0.1205266678  1  1  0  1  0 #> 128 -0.1723679990  0  0  1  1  1 #> 129  0.0823549821  0  0  1  1  0 #> 130  0.0805914564  1  0  1  0  1 #> 131 -0.1978631622  0  1  0  0  0 #> 132 -0.0005349054  1  0  1  0  0 #> 133  0.8448169996  0  0  1  0  0 #> 134 -0.1343452325  0  0  0  1  1 #> 135  0.4488476618  0  1  1  1  1 #> 136  0.2748160495  0  1  0  1  1 #> 137  0.2079566028  1  1  1  0  1 #> 138 -0.1336638343  1  0  1  0  0 #> 139  0.2469264903  0  1  1  0  0 #> 140  0.3855105100  0  1  1  1  0 #> 141  0.2140559227  0  0  1  1  1 #> 142  0.0700871731  0  1  0  0  0 #> 143  0.8108545496  1  0  0  0  0 #> 144  0.2166929222  1  0  0  0  0 #> 145  1.3390181875  1  0  0  1  0 #> 146  0.5456515402  1  1  0  0  0 #> 147 -0.3318034211  1  1  1  1  0 #> 148  0.8911191771  1  0  1  0  0 #> 149  0.1317889141  0  0  0  0  0 #> 150 -0.0743895794  0  0  0  0  0 #>  #> $housekeeping #>      n mu_i      theta_i #> 1   44  0.5  0.751593116 #> 2   30  0.0 -0.333319987 #> 3   36  0.5  0.259946504 #> 4   42  0.0  0.203900854 #> 5    8  0.5  0.333272677 #> 6   48  0.0 -0.101590063 #> 7   50  0.5  0.500086117 #> 8   46  0.5  0.575034492 #> 9   52  0.5  0.523972886 #> 10  66  0.0 -0.245329443 #> 11  50  0.5  0.192303599 #> 12  42  0.5  0.305490443 #> 13  42  0.0 -0.024421633 #> 14  36  0.5  0.549976149 #> 15  24  0.5  0.431615110 #> 16  30  0.0 -0.252439422 #> 17  42  0.5  0.479428248 #> 18  16  0.0  0.002462566 #> 19  60  0.0 -0.115571313 #> 20  44  0.5  0.916370461 #> 21  44  0.0  0.076819223 #> 22  40  0.0  0.031935406 #> 23  18  0.5  0.855925180 #> 24  34  0.5  0.544843473 #> 25  56  0.5  0.339226445 #> 26  32  0.0 -0.022609364 #> 27  54  0.5  0.940078186 #> 28  52  0.5  0.766181107 #> 29  50  0.5  0.331731433 #> 30  56  0.0 -0.116785319 #> 31  42  0.5  0.164752731 #> 32  18  0.5  0.632419021 #> 33  44  0.5  0.358384228 #> 34  18  0.5  0.394927198 #> 35  40  0.5  0.322318975 #> 36  50  0.0 -0.116699700 #> 37  38  0.0  0.042058371 #> 38  36  0.0 -0.414291477 #> 39  34  0.0 -0.134362368 #> 40  60  0.5  0.648517760 #> 41  34  0.5  0.616871721 #> 42  82  0.0  0.022864382 #> 43  58  0.5  0.263974696 #> 44  38  0.0  0.002218353 #> 45  56  0.5  0.508134057 #> 46  30  0.0 -0.339267568 #> 47  40  0.0 -0.437684389 #> 48  38  0.0  0.365781498 #> 49  22  0.5  0.851941905 #> 50  42  0.0 -0.165840739 #> 51  18  0.5  0.444477451 #> 52  32  0.5  0.529667395 #> 53  38  0.0  0.034483966 #> 54  40  0.0 -0.157414388 #> 55  54  0.0 -0.364823301 #> 56  46  0.5  0.680246853 #> 57  34  0.5  0.676100483 #> 58  28  0.0 -0.244393314 #> 59  44  0.0  0.328934539 #> 60  54  0.0  0.311210623 #> 61  56  0.5  0.455421861 #> 62  50  0.5  0.424529812 #> 63  10  0.5  0.575313592 #> 64  30  0.0  0.248007282 #> 65  28  0.5  0.621103649 #> 66  38  0.5  0.674340196 #> 67  46  0.0  0.497120688 #> 68  58  0.0 -0.261939925 #> 69  40  0.0 -0.339623134 #> 70  42  0.5  0.581938491 #> 71  28  0.5  0.454697744 #> 72  56  0.5  0.430230952 #> 73  34  0.5  0.601482092 #> 74  54  0.5  1.016411035 #> 75  62  0.5  0.455983640 #> 76  30  0.0  0.323725024 #> 77  78  0.5  0.550542960 #> 78  42  0.5  0.253060913 #> 79  50  0.0  0.176795679 #> 80  26  0.5  0.771009613 #> 81  54  0.0  0.129566696 #> 82  46  0.0 -0.116406274 #> 83  34  0.0  0.069236541 #> 84  36  0.0 -0.194299499 #> 85  32  0.5  0.781030782 #> 86  62  0.0  0.343158794 #> 87  42  0.0  0.166258046 #> 88  50  0.5  0.702321785 #> 89  34  0.5  0.464791569 #> 90  42  0.0  0.210350910 #> 91  42  0.0  0.003151477 #> 92  46  0.0  0.008161278 #> 93  30  0.0  0.074808844 #> 94  32  0.5  0.536789276 #> 95  14  0.5  0.777536531 #> 96  28  0.0  0.382863686 #> 97  46  0.0 -0.111754255 #> 98  38  0.5  0.694209077 #> 99  40  0.0 -0.165236592 #> 100 42  0.5  0.721944321 #> 101 36  0.5  0.504473660 #> 102 32  0.5  0.733613486 #> 103 46  0.0 -0.094132651 #> 104 24  0.0 -0.018162156 #> 105 50  0.5  0.370018449 #> 106 54  0.5  0.451048736 #> 107 34  0.5  0.367677401 #> 108 56  0.0  0.024588526 #> 109 48  0.5  0.315783208 #> 110 42  0.0 -0.153688502 #> 111 22  0.5  0.751588557 #> 112 28  0.5  0.440816462 #> 113 46  0.0 -0.241672109 #> 114 50  0.5  0.528996773 #> 115 48  0.5  0.565896920 #> 116 28  0.0 -0.242492584 #> 117 34  0.0 -0.025367043 #> 118 38  0.0  0.122714748 #> 119 56  0.5  0.123237904 #> 120 56  0.5  0.555290144 #> 121 20  0.5  0.708371544 #> 122 42  0.5  0.371526301 #> 123 56  0.0  0.082036300 #> 124 28  0.0  0.103376035 #> 125 30  0.5  0.517528926 #> 126 18  0.5  0.624071906 #> 127 26  0.5  0.708166163 #> 128 26  0.0  0.045687466 #> 129 60  0.0  0.128322816 #> 130 30  0.5  0.456718548 #> 131 48  0.0  0.064661647 #> 132 20  0.5  0.360405969 #> 133 40  0.0  0.117729175 #> 134 66  0.0 -0.050175225 #> 135 44  0.0 -0.036823966 #> 136 28  0.0 -0.017780269 #> 137 70  0.5  0.179446748 #> 138 42  0.5  0.413166195 #> 139 56  0.0  0.262883275 #> 140 44  0.0  0.451997423 #> 141 22  0.0  0.362111531 #> 142 44  0.0 -0.154110854 #> 143 66  0.5  0.371361599 #> 144 28  0.5  0.315437282 #> 145 22  0.5  0.900416075 #> 146 36  0.5  0.542047973 #> 147 46  0.5  0.240674876 #> 148 48  0.5  0.384783500 #> 149 14  0.0 -0.073030411 #> 150 40  0.0  0.029996208 #>  #> $tau2_est #> [1] 0.09684336 #>  simulate_smd(distribution = \"bernoulli\", model = \"es * x[ ,1] * x[ ,2]\") #> $training #>            vi           yi X1 X2 X3 X4 X5 #> 1  0.06299004  0.101690648  0  0  1  0  1 #> 2  0.07851046 -0.316829310  0  1  1  0  0 #> 3  0.06494661  0.006000258  1  1  0  0  1 #> 4  0.06248350  0.437081405  1  1  1  1  1 #> 5  0.08151884  0.633419001  1  1  0  0  0 #> 6  0.08845043 -0.261689473  1  0  1  1  1 #> 7  0.11223374  0.073369528  1  1  0  1  0 #> 8  0.07377923  0.445377026  1  0  0  0  1 #> 9  0.13569134 -0.240723218  1  0  1  0  0 #> 10 0.09745917 -0.333850759  1  0  1  0  1 #> 11 0.10503473  0.561055136  1  1  0  1  0 #> 12 0.08011055 -0.510284534  0  0  1  1  0 #> 13 0.08773877 -0.076520868  0  1  1  0  0 #> 14 0.11216053  0.020116090  1  0  0  1  0 #> 15 0.09195915  0.153647028  1  0  0  0  0 #> 16 0.09752529  0.700830454  0  1  0  1  0 #> 17 0.20205706  0.133484765  0  0  1  0  0 #> 18 0.08863228 -0.290661784  0  1  1  1  1 #> 19 0.07446868  0.749793883  1  1  0  1  0 #> 20 0.07985105  0.737493474  1  1  1  0  0 #>  #> $testing #>               yi X1 X2 X3 X4 X5 #> 21   0.043815464  0  0  1  1  0 #> 22  -0.048966725  0  1  0  0  1 #> 23  -0.392490028  0  0  1  1  1 #> 24   0.433750412  0  0  1  1  1 #> 25  -0.140216911  1  0  0  1  1 #> 26   0.297429275  1  1  1  0  0 #> 27   0.376034576  1  1  1  0  1 #> 28   0.582961055  1  0  0  1  0 #> 29   0.609602364  0  0  1  1  1 #> 30   0.061378434  0  0  0  0  1 #> 31   0.490004390  1  1  1  1  0 #> 32   0.102490938  1  1  0  1  1 #> 33   0.277943674  1  1  0  1  1 #> 34   0.750090467  1  1  0  1  1 #> 35  -0.126124106  0  0  0  1  0 #> 36  -0.134621588  1  1  0  1  1 #> 37   0.281455564  0  0  1  1  1 #> 38   0.024912072  0  0  0  1  0 #> 39  -0.246580973  1  0  1  1  0 #> 40  -0.210515066  0  1  1  1  1 #> 41   0.329798576  0  0  1  1  0 #> 42   0.129320549  0  1  1  1  1 #> 43  -0.318387047  0  1  1  0  1 #> 44   0.321880461  0  1  1  0  0 #> 45  -0.014817962  1  0  0  1  0 #> 46  -0.013445415  0  0  1  1  0 #> 47  -0.383711975  1  0  1  0  0 #> 48   0.017690162  1  0  0  0  1 #> 49   0.064626566  1  1  0  0  0 #> 50   0.136806194  1  1  0  0  1 #> 51   0.233782428  0  1  0  1  0 #> 52   0.429324364  0  1  0  0  1 #> 53   0.546178309  1  1  0  0  0 #> 54  -1.046527040  1  0  0  0  1 #> 55   0.130567846  0  1  0  1  0 #> 56   0.568800820  1  1  1  1  0 #> 57   0.131183163  0  0  1  1  1 #> 58  -0.274906568  0  0  0  1  0 #> 59  -0.242326268  0  0  0  0  0 #> 60   0.282678008  0  1  1  0  1 #> 61   0.156371610  0  1  1  1  0 #> 62   0.566442533  1  0  0  0  1 #> 63   0.735351939  1  1  1  1  1 #> 64  -0.290157779  0  1  0  1  1 #> 65   0.358903351  0  0  1  0  1 #> 66  -0.517552114  1  0  0  0  0 #> 67  -0.214688133  0  1  1  1  0 #> 68   0.350995191  0  1  0  1  1 #> 69  -0.008986599  1  1  1  0  1 #> 70   0.175530772  0  1  0  0  0 #> 71  -0.111710942  1  0  0  1  1 #> 72   0.069247766  0  1  0  0  0 #> 73   0.040908692  0  0  0  1  0 #> 74  -0.654383762  0  1  0  0  1 #> 75   0.239617380  1  0  1  1  1 #> 76   0.328919754  0  1  0  0  0 #> 77   0.393856139  0  1  1  1  1 #> 78   0.843287275  1  1  0  1  1 #> 79  -0.443871361  0  0  0  1  0 #> 80  -0.096871180  0  1  0  0  1 #> 81  -0.406524790  0  1  1  1  1 #> 82   0.428544929  1  0  0  0  0 #> 83  -0.045226124  0  0  0  1  1 #> 84  -0.312313647  1  0  0  0  0 #> 85  -0.528063152  1  1  1  1  0 #> 86   0.677445564  1  1  1  1  0 #> 87  -0.109778764  1  0  1  0  0 #> 88   0.397106369  1  1  0  1  0 #> 89   0.041568034  1  0  0  1  1 #> 90  -0.102783661  0  1  0  1  1 #> 91   0.215353291  0  0  1  1  0 #> 92  -0.139109751  0  0  0  1  0 #> 93   0.075468378  1  0  1  1  0 #> 94   0.749387268  1  1  0  1  0 #> 95  -0.263918561  0  0  1  0  1 #> 96   0.304536926  1  0  0  1  0 #> 97   0.524325007  1  1  0  1  0 #> 98   0.294376356  1  1  0  1  0 #> 99  -0.625542875  0  0  1  0  0 #> 100  0.154216232  0  0  1  0  1 #> 101 -0.459311288  0  1  0  1  1 #> 102 -0.707305530  1  1  1  0  0 #> 103  0.605729758  0  0  1  0  1 #> 104  0.403280400  0  1  0  0  0 #> 105 -0.174587977  0  1  1  1  0 #> 106 -0.791154047  0  1  1  0  0 #> 107  0.055850735  0  1  1  1  1 #> 108  0.457637433  0  1  1  0  1 #> 109 -0.583514471  1  0  0  1  0 #> 110 -0.206441167  0  0  0  1  0 #> 111  1.139266569  1  1  0  0  1 #> 112  1.244348667  1  1  0  0  0 #> 113 -0.090969869  0  1  1  1  0 #> 114 -0.227222012  1  0  0  1  0 #> 115 -0.381972955  1  0  0  1  1 #> 116  0.412649206  1  0  0  0  1 #> 117  0.172336439  1  0  1  0  0 #> 118  0.643561153  0  0  1  1  1 #> 119  0.407833256  0  0  0  0  0 #> 120 -0.189808725  0  1  1  1  1 #>  #> $housekeeping #>      n mu_i      theta_i #> 1   62  0.0 -0.195914572 #> 2   50  0.0  0.117754379 #> 3   60  0.5  0.504442832 #> 4   64  0.5  0.334986895 #> 5   50  0.5  0.635909615 #> 6   44  0.0 -0.166799419 #> 7   34  0.5  0.348473153 #> 8   54  0.0  0.263732698 #> 9   28  0.0 -0.015865433 #> 10  40  0.0 -0.145624750 #> 11  38  0.5  0.653275533 #> 12  50  0.0 -0.065551359 #> 13  44  0.0 -0.101508381 #> 14  34  0.0  0.178484836 #> 15  42  0.0  0.016898242 #> 16  42  0.0 -0.037448007 #> 17  18  0.0  0.063490797 #> 18  44  0.0  0.028640294 #> 19  56  0.5  0.583168654 #> 20  52  0.5  0.708355141 #> 21  14  0.0  0.112886227 #> 22  28  0.0 -0.052430244 #> 23  50  0.0 -0.177821497 #> 24  40  0.0  0.301221132 #> 25  16  0.0 -0.031030426 #> 26  22  0.5  0.374363232 #> 27  44  0.5  0.592962634 #> 28  46  0.0  0.235904218 #> 29  26  0.0  0.119646973 #> 30  44  0.0  0.132285001 #> 31  52  0.5  0.592947727 #> 32  48  0.5  0.639643034 #> 33  30  0.5  0.213354069 #> 34  40  0.5  0.766491423 #> 35  44  0.0 -0.118927588 #> 36  22  0.5  0.505247036 #> 37  60  0.0  0.107720332 #> 38  42  0.0 -0.264762986 #> 39  28  0.0  0.090561793 #> 40  46  0.0  0.129762741 #> 41  30  0.0 -0.058243852 #> 42  48  0.0  0.050742996 #> 43  60  0.0 -0.002325187 #> 44  40  0.0  0.027255662 #> 45  34  0.0 -0.090533400 #> 46  74  0.0  0.139379741 #> 47  46  0.0 -0.084093802 #> 48  44  0.0 -0.098884221 #> 49  54  0.5  0.485284650 #> 50  24  0.5  0.288016313 #> 51  52  0.0  0.061619277 #> 52  24  0.0 -0.086395939 #> 53  48  0.5  0.537067726 #> 54  26  0.0 -0.369489297 #> 55  62  0.0 -0.025679440 #> 56  62  0.5  0.517027953 #> 57  28  0.0 -0.128003046 #> 58  52  0.0  0.165316992 #> 59  44  0.0  0.088843384 #> 60  50  0.0 -0.081190727 #> 61  38  0.0  0.030984774 #> 62  66  0.0  0.172610146 #> 63  30  0.5  0.374923318 #> 64  34  0.0  0.023183774 #> 65  28  0.0 -0.095491210 #> 66  36  0.0 -0.047627377 #> 67  42  0.0 -0.070378435 #> 68  34  0.0  0.199668073 #> 69  22  0.5  0.310584248 #> 70  14  0.0 -0.179261501 #> 71  50  0.0 -0.200384649 #> 72  80  0.0 -0.130594376 #> 73  56  0.0  0.018470968 #> 74  46  0.0 -0.008432395 #> 75  36  0.0  0.263752308 #> 76  44  0.0  0.203247060 #> 77  50  0.0  0.128966290 #> 78  52  0.5  0.566950146 #> 79  20  0.0 -0.041844479 #> 80  44  0.0  0.157175036 #> 81  32  0.0  0.088085262 #> 82  28  0.0 -0.187514419 #> 83  28  0.0 -0.044539113 #> 84  14  0.0 -0.170822472 #> 85  26  0.5  0.125015758 #> 86  26  0.5  0.658994613 #> 87  24  0.0 -0.274062994 #> 88  32  0.5  0.849156412 #> 89  32  0.0 -0.124885419 #> 90  60  0.0 -0.122154816 #> 91  56  0.0  0.449999010 #> 92  50  0.0  0.227325783 #> 93  60  0.0 -0.062310670 #> 94  64  0.5  0.566832273 #> 95  34  0.0 -0.047588972 #> 96  48  0.0 -0.307326853 #> 97  42  0.5  0.524976277 #> 98  36  0.5  0.514501304 #> 99  28  0.0 -0.089717080 #> 100 34  0.0 -0.022378912 #> 101 36  0.0 -0.284214283 #> 102 30  0.5  0.093212443 #> 103 26  0.0  0.299978974 #> 104 38  0.0  0.255503634 #> 105 20  0.0 -0.348699761 #> 106 36  0.0 -0.114085754 #> 107 34  0.0  0.015713373 #> 108 54  0.0  0.213715345 #> 109 38  0.0 -0.131536879 #> 110 54  0.0 -0.045867605 #> 111 46  0.5  0.903520519 #> 112 40  0.5  0.781926271 #> 113 36  0.0 -0.401014680 #> 114 36  0.0  0.133779366 #> 115 64  0.0 -0.275685450 #> 116 50  0.0  0.311662217 #> 117 34  0.0  0.227026447 #> 118 52  0.0  0.126597864 #> 119 34  0.0  0.280484974 #> 120 30  0.0 -0.170053996 #>  #> $tau2_est #> [1] 0.07936629 #>"}]
